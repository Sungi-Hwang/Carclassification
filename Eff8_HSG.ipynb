{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "1cRA_Nj6a-Yiu_yPR_1nV2aE1xx1yNi2N",
      "authorship_tag": "ABX9TyN2a4qy6qTFeH4dSc4xyLjW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sungi-Hwang/Carclassification/blob/main/Eff8_HSG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, random, unicodedata, numpy as np, torch, gc\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.cuda.amp import autocast\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torchvision import transforms\n",
        "from torchvision.models import efficientnet_b7, EfficientNet_B7_Weights\n",
        "from torchvision.datasets.folder import IMG_EXTENSIONS\n",
        "from PIL import Image, ImageFile; ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import log_loss\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "1IedKvjnLdaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 구글 드라이브 마운트\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# # 압축 해제\n",
        "# !unzip -oq \"/content/drive/MyDrive/Colab Notebooks/open.zip\" -d /content/"
      ],
      "metadata": {
        "id": "C6Fu6e3pi9BR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# ─── 1. CFG & Seed ──────────────────────────────────────────────\n",
        "CFG = dict(IMG_SIZE=600, BATCH_SIZE=128, EPOCHS=10,\n",
        "           LEARNING_RATE=1e-4, SEED=42,\n",
        "           NUM_W=min(os.cpu_count(), 32))\n",
        "def seed_all(s):\n",
        "    random.seed(s); np.random.seed(s)\n",
        "    torch.manual_seed(s); torch.cuda.manual_seed(s)\n",
        "seed_all(CFG['SEED'])\n",
        "# ─── 2. Dataset ─────────────────────────────────────────────────\n",
        "MERGE_MAP = {'K5_하이브리드_3세대_2020_2023':'K5_3세대_하이브리드_2020_2022',\n",
        "             '디_올_뉴_니로_2022_2025':'디_올뉴니로_2022_2025',\n",
        "             '박스터_718_2017_2024':'718_박스터_2017_2024'}\n",
        "MERGE_MAP = {unicodedata.normalize(\"NFC\",k):\n",
        "             unicodedata.normalize(\"NFC\",v) for k,v in MERGE_MAP.items()}\n",
        "\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, root, transform):\n",
        "        self.transform, self.samples = transform, []\n",
        "        cls_dirs = [d for d in os.scandir(root) if d.is_dir()]\n",
        "        cls_dirs = [unicodedata.normalize(\"NFC\",d.name) for d in cls_dirs]\n",
        "        unified  = {c: MERGE_MAP.get(c,c) for c in cls_dirs}\n",
        "        self.classes = sorted(set(unified.values()))\n",
        "        self.cls2idx = {c:i for i,c in enumerate(self.classes)}\n",
        "        for orig in cls_dirs:\n",
        "            lbl = self.cls2idx[unified[orig]]\n",
        "            for f in os.scandir(os.path.join(root,orig)):\n",
        "                if f.is_file() and f.name.lower().endswith(tuple(IMG_EXTENSIONS)):\n",
        "                    self.samples.append((f.path,lbl))\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self, i):\n",
        "        p,l = self.samples[i]\n",
        "        img  = self.transform(Image.open(p).convert('RGB'))\n",
        "        return img,l\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "TWsvfrLcVFPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import EfficientNet_B7_Weights\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# 1️⃣  기본 세팅 ----------------------------------------------------\n",
        "weights   = EfficientNet_B7_Weights.DEFAULT        # == IMAGENET1K_V1\n",
        "base_tf   = weights.transforms()                   # Resize → CenterCrop → ToTensor → Normalize\n",
        "\n",
        "IMG_SIZE  = CFG['IMG_SIZE']                        # ex) 600 for B7\n",
        "BATCH_SZ  = CFG['BATCH_SIZE']\n",
        "\n",
        "# 2️⃣  Train / Val 전처리 -------------------------------------------\n",
        "train_tf = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.7, 1.0), antialias=True),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    *base_tf.transforms                           # 표준화까지 한꺼번에\n",
        "])\n",
        "\n",
        "val_tf = transforms.Compose([\n",
        "    transforms.Resize(int(IMG_SIZE * 1.15), antialias=True),\n",
        "    transforms.CenterCrop(IMG_SIZE),\n",
        "    *base_tf.transforms\n",
        "])\n",
        "\n",
        "# 3️⃣  데이터셋 & 로더 -----------------------------------------------\n",
        "root = \"/content/train\"\n",
        "ds_full   = CustomImageDataset(root, transform=train_tf)\n",
        "ds_full_v = CustomImageDataset(root, transform=val_tf)   # 샘플 순서 동일, TF만 다름\n",
        "\n",
        "targets = [lbl for _, lbl in ds_full.samples]\n",
        "tr_idx, v_idx = train_test_split(\n",
        "    np.arange(len(ds_full)),\n",
        "    test_size=0.20,\n",
        "    stratify=targets,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    Subset(ds_full, tr_idx),\n",
        "    batch_size=BATCH_SZ,\n",
        "    shuffle=True,\n",
        "    num_workers=CFG['NUM_W'],\n",
        "    pin_memory=True,\n",
        "    persistent_workers=True,\n",
        "    prefetch_factor=4\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    Subset(ds_full_v, v_idx),\n",
        "    batch_size=BATCH_SZ,\n",
        "    shuffle=False,\n",
        "    num_workers=max(1, CFG['NUM_W'] // 2),\n",
        "    pin_memory=True,\n",
        "    persistent_workers=True\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "0RJCL3H_BK9a",
        "outputId": "6cf17443-3b8f-4a34-9cec-2ae4fca9e9c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'ImageClassification' object has no attribute 'transforms'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-d8833c65241a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomResizedCrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mantialias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomHorizontalFlip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;34m*\u001b[0m\u001b[0mbase_tf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m                           \u001b[0;31m# 표준화까지 한꺼번에\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m ])\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1930\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1931\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m   1932\u001b[0m             \u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1933\u001b[0m         )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'ImageClassification' object has no attribute 'transforms'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ─── 4. Model ───────────────────────────────────────────────────\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32      = True\n",
        "AMP = torch.bfloat16\n",
        "device = torch.device('cuda')\n",
        "\n",
        "class BaseModel(nn.Module):\n",
        "    def __init__(self,nc):\n",
        "        super().__init__()\n",
        "        self.backbone = efficientnet_b7(weights=w)\n",
        "        in_f = self.backbone.classifier[1].in_features\n",
        "        self.backbone.classifier[1] = nn.Linear(in_f, nc)\n",
        "    def forward(self,x): return self.backbone(x)\n",
        "\n",
        "model = BaseModel(len(ds_full.classes)).to(device,\n",
        "        memory_format=torch.channels_last, dtype=AMP)\n",
        "model = torch.compile(model, mode='max-autotune')\n",
        "\n",
        "opt = torch.optim.AdamW(model.parameters(),\n",
        "        lr=CFG['LEARNING_RATE'], fused=True)\n",
        "crit = nn.CrossEntropyLoss(); best = np.inf"
      ],
      "metadata": {
        "id": "9SZFF9CxYcF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cxOstToSpW7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IqneTmJbaMXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ─── 5. Train / Val ─────────────────────────────────────────────\n",
        "for ep in range(CFG['EPOCHS']):\n",
        "    # train\n",
        "    model.train(); tl=0\n",
        "    for x,y in tqdm(train_loader, desc=f\"Ep{ep+1} Train\"):\n",
        "        x = x.to(device, memory_format=torch.channels_last); y=y.to(device)\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        with autocast(dtype=AMP):\n",
        "            out = model(x); loss = crit(out,y)\n",
        "        loss.backward(); opt.step(); tl+=loss.item()\n",
        "    # val\n",
        "    model.eval(); all_p, all_y = [], []; correct=total=0\n",
        "    with torch.no_grad():\n",
        "        for x,y in tqdm(val_loader, desc=f\"Ep{ep+1} Val\"):\n",
        "            x=x.to(device, memory_format=torch.channels_last); y=y.to(device)\n",
        "            with autocast(dtype=AMP):\n",
        "                out=model(x); loss=crit(out,y)\n",
        "            prob = F.softmax(out,1)\n",
        "            correct += (prob.argmax(1)==y).sum().item(); total+=y.size(0)\n",
        "            all_p.append(prob.cpu()); all_y.append(y.cpu())\n",
        "    logloss = log_loss(torch.cat(all_y), torch.cat(all_p))\n",
        "    acc = 100*correct/total\n",
        "    print(f\"Ep{ep+1}  logloss={logloss:.4f}  acc={acc:.2f}%\")\n",
        "    if logloss < best:\n",
        "        best = logloss\n",
        "        torch.save({k:v.cpu() for k,v in model.state_dict().items()},\n",
        "                   'best_model.pth')\n",
        "        print(\"  ✅ best_model.pth saved\")"
      ],
      "metadata": {
        "id": "zdECrE8zZXnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pVdQDCJBYv3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cknC4-7WYyj1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}