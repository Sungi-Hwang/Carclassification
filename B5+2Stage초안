{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "1cG8623QeyeYiYGSZ5WT1rHVBe4nCokik",
      "authorship_tag": "ABX9TyNdAEXfnG8IcEyNEujMvXn4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "54edd91cb3234752a4b6e94932efb7d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_196f46de6b1e46da95b92448780b8e1c",
              "IPY_MODEL_3e898f0791484fd9aaac886828f9168f",
              "IPY_MODEL_0c03fc4a4f2c4438a0b8341112a31b0c"
            ],
            "layout": "IPY_MODEL_81fe852edc7f4bf994049a40fdfe5bd8"
          }
        },
        "196f46de6b1e46da95b92448780b8e1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05092434c3f24220a4a59fa45b13c819",
            "placeholder": "​",
            "style": "IPY_MODEL_35314ad301f34e11879a9110007009c6",
            "value": "model.safetensors: 100%"
          }
        },
        "3e898f0791484fd9aaac886828f9168f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83cad76172624e458c14f56becdf1d3b",
            "max": 122330162,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fcc10c44e092413c80560d8d453cfba7",
            "value": 122330162
          }
        },
        "0c03fc4a4f2c4438a0b8341112a31b0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82be077dbea94da883045ba0eae50070",
            "placeholder": "​",
            "style": "IPY_MODEL_3cb6c7a8f0214f3b9d92a686307172bc",
            "value": " 122M/122M [00:00&lt;00:00, 248MB/s]"
          }
        },
        "81fe852edc7f4bf994049a40fdfe5bd8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05092434c3f24220a4a59fa45b13c819": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35314ad301f34e11879a9110007009c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "83cad76172624e458c14f56becdf1d3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcc10c44e092413c80560d8d453cfba7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "82be077dbea94da883045ba0eae50070": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cb6c7a8f0214f3b9d92a686307172bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sungi-Hwang/Carclassification/blob/main/B5%2B2Stage%EC%B4%88%EC%95%88\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import zipfile\n",
        "\n",
        "# 1. Drive에서 로컬로 복사 (빠름)\n",
        "shutil.copy(\"/content/drive/MyDrive/open.zip\", \"/content/open.zip\")\n",
        "\n",
        "# 2. 압축 풀기\n",
        "with zipfile.ZipFile(\"/content/open.zip\", \"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"/content/\")\n",
        "\n",
        "# 3. 확인\n",
        "!ls /content/train\n"
      ],
      "metadata": {
        "id": "VGLGwzmFf9U8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f76cc8a-459b-4db0-f1c7-ae6b021ba1ec"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "컨티넨탈_10세대_2017_2019\t     A_클래스_W177_2020_2025\n",
            "어코드_10세대_2018_2022\t\t     B_클래스_W246_2013_2018\n",
            "1시리즈_F20_2013_2015\t\t     뉴_스타일_코란도_C_2017_2019\n",
            "1시리즈_F20_2016_2019\t\t     프리우스_C_2018_2020\n",
            "1시리즈_F40_2020_2024\t\t     뉴_CC_2012_2016\n",
            "그랜드카니발_2006_2010\t\t     CLA_클래스_C117_2014_2019\n",
            "2008_2015_2017\t\t\t     CLA_클래스_C118_2020_2025\n",
            "에쿠스_신형_2010_2015\t\t     CLE_클래스_C236_2024_2025\n",
            "파나메라_2010_2016\t\t     CLS_클래스_C257_2019_2023\n",
            "뉴_제타_2011_2016\t\t     CLS_클래스_W218_2012_2017\n",
            "뉴_카이엔_2011_2018\t\t     아반떼_하이브리드_CN7_2021_2023\n",
            "엑센트_신형_2011_2019\t\t     아반떼_CN7_2021_2023\n",
            "스파크_2012_2015\t\t     더_뉴_아반떼_CN7_2023_2025\n",
            "쿠퍼_컨트리맨_2012_2015\t\t     CT6_2016_2018\n",
            "올_뉴_모닝_2012_2015\t\t     C_클래스_W204_2008_2015\n",
            "아베오_2012_2016\t\t     C_클래스_W205_2015_2021\n",
            "말리부_2012_2016\t\t     C_클래스_W206_2022_2024\n",
            "뉴_티구안_2012_2016\t\t     제네시스_DH_2014_2016\n",
            "레이_2012_2017\t\t\t     쏘나타_DN8_2020_2023\n",
            "올란도_2012_2018\t\t     쏘나타_디_엣지_DN8_2024_2025\n",
            "더_뉴_파사트_2012_2019\t\t     e_트론_2020_2023\n",
            "트랙스_2013_2016\t\t     E_PACE_2018_2020\n",
            "콰트로포르테_2014_2016\t\t     EQ900_2016_2018\n",
            "더_뉴_아반떼_2014_2016\t\t     EQA_H243_2021_2024\n",
            "마칸_2014_2018\t\t\t     EQE_V295_2022_2024\n",
            "그랜드_체로키_2014_2020\t\t     EQS_V297_2022_2023\n",
            "기블리_2014_2023\t\t     뉴_ES300h_2013_2015\n",
            "더_뉴_모닝_2015_2016\t\t     뉴_ES300h_2016_2018\n",
            "레니게이드_2015_2017\t\t     ES300h_7세대_2019_2026\n",
            "올_뉴_쏘렌토_2015_2017\t\t     디_올뉴니로EV_2023_2024\n",
            "아슬란_2015_2018\t\t     더_기아_레이_EV_2024_2025\n",
            "티볼리_2015_2018\t\t     EV6_2022_2024\n",
            "디스커버리_스포츠_2015_2019\t     EV9_2024_2025\n",
            "올_뉴_카니발_2015_2019\t\t     E_클래스_W212_2010_2016\n",
            "에스컬레이드_2015_2020\t\t     E_클래스_W213_2017_2020\n",
            "머스탱_2015_2023\t\t     E_클래스_W213_2021_2023\n",
            "익스플로러_2016_2017\t\t     E_클래스_W214_2024_2025\n",
            "그랜드_스타렉스_2016_2018\t     F150_2004_2021\n",
            "싼타페_더_프라임_2016_2018\t     F_PACE_2017_2019\n",
            "더_넥스트_스파크_2016_2018\t     G4_렉스턴_2018_2020\n",
            "더_뉴_맥스크루즈_2016_2018\t     G70_2018_2020\n",
            "더_뉴_코란도_스포츠_2016_2018\t     더_뉴_G70_2021_2025\n",
            "레인지로버_이보크_2016_2019\t     G80_2017_2020\n",
            "아이오닉_하이브리드_2016_2019\t     더_올뉴G80_2021_2024\n",
            "티볼리_에어_2016_2019\t\t     뉴_G80_2025_2026\n",
            "임팔라_2016_2019\t\t     G80_RG3_2021_2023\n",
            "쿠퍼_컨트리맨_2016_2024\t\t     G80_RG3_2025\n",
            "쿠퍼_컨버터블_2016_2024\t\t     G90_2019_2022\n",
            "쿠퍼_클럽맨_2016_2024\t\t     G90_RS4_2022_2025\n",
            "알티마_2017_2018\t\t     GLA_클래스_H247_2020_2025\n",
            "올_뉴_카마로_2017_2018\t\t     GLA_클래스_X156_2015_2019\n",
            "올_뉴_말리부_2017_2018\t\t     GLB_클래스_X247_2020_2023\n",
            "니로_2017_2019\t\t\t     GLC_클래스_X253_2017_2019\n",
            "더_뉴_모하비_2017_2019\t\t     GLC_클래스_X253_2020_2022\n",
            "콰트로포르테_2017_2022\t\t     GLC_클래스_X253_2023\n",
            "르반떼_2017_2022\t\t     GLC_클래스_X254_2023_2025\n",
            "더_뉴_트랙스_2017_2022\t\t     GLE_클래스_W166_2016_2018\n",
            "레인지로버_벨라_2018_2019\t     GLE_클래스_W167_2019_2024\n",
            "익스플로러_2018_2019\t\t     GLS_클래스_X166_2017_2019\n",
            "티볼리_아머_2018_2019\t\t     GLS_클래스_X167_2020_2024\n",
            "쏘나타_뉴_라이즈_2018_2019\t     그랜저_GN7_2023_2025\n",
            "스토닉_2018_2020\t\t     컨티넨탈_GT_2세대_2012_2017\n",
            "스팅어_2018_2020\t\t     컨티넨탈_GT_3세대_2018_2023\n",
            "코나_2018_2020\t\t\t     파사트_GT_B8_2018_2022\n",
            "더_뉴_쏘렌토_2018_2020\t\t     GV70_2021_2023\n",
            "렉스턴_스포츠_2018_2021\t\t     일렉트리파이드_GV70_2022_2024\n",
            "더_뉴_그랜드_스타렉스_2018_2021      GV80_2020_2022\n",
            "더_뉴_레이_2018_2022\t\t     뉴_GV80_2024_2025\n",
            "티구안_올스페이스_2018_2023\t     GV80_2024_2025\n",
            "아테온_2018_2023\t\t     G_클래스_W463_2009_2017\n",
            "넥쏘_2018_2024\t\t\t     G_클래스_W463b_2019_2025\n",
            "렉스턴_스포츠_칸_2019_2020\t     그랜저_HG_2011_2014\n",
            "더_뉴_카니발_2019_2020\t\t     그랜저_HG_2015_2017\n",
            "마칸_2019_2021\t\t\t     i30_PD_2017_2018\n",
            "팰리세이드_2019_2022\t\t     i4_2022_2024\n",
            "스포티지_더_볼드_2019_2022\t     그랜저_IG_2017_2019\n",
            "더_뉴_말리부_2019_2022\t\t     더_뉴_그랜저_IG_2020_2023\n",
            "더_뉴_스파크_2019_2022\t\t     iX_2022_2024\n",
            "레니게이드_2019_2023\t\t     올_뉴_모닝_JA_2017_2020\n",
            "뷰티풀_코란도_2019_2024\t\t     모닝_어반_JA_2021_2023\n",
            "더_뉴_아이오닉_하이브리드_2020\t     더_뉴_모닝_JA_2024_2025\n",
            "콜로라도_2020_2020\t\t     랭글러_JK_2009_2017\n",
            "코세어_2020_2022\t\t     랭글러_JL_2018_2024\n",
            "더_뉴_니로_2020_2022\t\t     벨로스터_JS_2018_2020\n",
            "트래버스_2020_2023\t\t     글래디에이터_JT_2020_2023\n",
            "셀토스_2020_2023\t\t     K3_2013_2015\n",
            "베리_뉴_티볼리_2020_2023\t     더_뉴_K3_2016_2018\n",
            "모하비_더_마스터_2020_2024\t     올_뉴_K3_2019_2021\n",
            "베뉴_2020_2024\t\t\t     더_뉴_K3_2세대_2022_2024\n",
            "트레일블레이저_2021_2022\t     K5_2세대_2016_2018\n",
            "티볼리_에어_2021_2022\t\t     더_뉴_K5_2세대_2019_2020\n",
            "리얼_뉴_콜로라도_2021_2022\t     K5_3세대_하이브리드_2020_2022\n",
            "스팅어_마이스터_2021_2023\t     K5_하이브리드_3세대_2020_2023\n",
            "더_올뉴투싼_하이브리드_2021_2023     K5_3세대_2020_2023\n",
            "더_뉴_싼타페_2021_2023\t\t     더_뉴_K5_하이브리드_3세대_2023_2025\n",
            "더_뉴_코나_2021_2023\t\t     더_뉴_K5_3세대_2024_2025\n",
            "타이칸_2021_2025\t\t     더_뉴_K7_2013_2016\n",
            "더_뉴_렉스턴_스포츠_칸_2021_2025     올_뉴_K7_2016_2019\n",
            "더_뉴_렉스턴_스포츠_2021_2025\t     올_뉴_K7_하이브리드_2017_2019\n",
            "올_뉴_렉스턴_2021_2025\t\t     K7_프리미어_하이브리드_2020_2021\n",
            "캐스퍼_2022_2024\t\t     K7_프리미어_2020_2021\n",
            "마칸_2022_2024\t\t\t     K8_하이브리드_2022_2024\n",
            "디_올_뉴_스포티지_2022_2024\t     K8_2022_2024\n",
            "스타리아_2022_2025\t\t     더_K9_2019_2021\n",
            "디_올뉴니로_2022_2025\t\t     더_뉴_K9_2세대_2022_2025\n",
            "더_뉴_기아_레이_2022_2025\t     체로키_KL_2019_2023\n",
            "디_올_뉴_니로_2022_2025\t\t     디펜더_L663_2020_2025\n",
            "트레일블레이저_2023\t\t     LF_쏘나타_2015_2017\n",
            "더_뉴_팰리세이드_2023_2024\t     팰리세이드_LX3_2025\n",
            "토레스_2023_2025\t\t     M2_F87_2016_2021\n",
            "디_올뉴그랜저_2023_2025\t\t     M4_F82_2015_2020\n",
            "디_올뉴코나_2023_2025\t\t     M5_F90_2018_2023\n",
            "더_뉴_셀토스_2023_2025\t\t     아반떼_MD_2011_2014\n",
            "트랙스_크로스오버_2024_2025\t     MKC_2015_2018\n",
            "디_올뉴싼타페_2024_2025\t\t     뉴_MKZ_2017_2020\n",
            "그랑_콜레오스_2025\t\t     싼타페_MX5_2024_2025\n",
            "레인지로버_스포츠_2세대_2013_2017    아반떼_N_2022_2023\n",
            "레인지로버_스포츠_2세대_2018_2022    New_XF_2012_2015\n",
            "컴패스_2세대_2018_2022\t\t     투싼_NX4_2021_2023\n",
            "레인지로버_이보크_2세대_2020_2022    더_뉴_투싼_NX4_2023_2025\n",
            "디스커버리_스포츠_2세대_2020_2025    카이엔_PO536_2019_2023\n",
            "에비에이터_2세대_2020_2025\t     Q30_2017_2019\n",
            "레인지로버_이보크_2세대_2023_2024    Q3_F3_2020_2024\n",
            "액티언_2세대_2025\t\t     Q50_2014_2017\n",
            "2시리즈_그란쿠페_F44_2020_2024\t     Q5_FY_2020\n",
            "2시리즈_액티브_투어러_F45_2019_2021  Q5_FY_2021_2024\n",
            "2시리즈_액티브_투어러_U06_2022_2024  Q7_4M_2016_2019\n",
            "3008_2세대_2018_2023\t\t     Q7_4M_2020_2023\n",
            "파일럿_3세대_2016_2018\t\t     Q8_4M_2020_2025\n",
            "모델_3_2019_2022\t\t     QM3_2014_2017\n",
            "투아렉_3세대_2020_2023\t\t     뉴QM3_2018_2019\n",
            "모델_3_2024_2025\t\t     뉴_QM5_2012_2014\n",
            "3시리즈_E90_2005_2012\t\t     QM6_2017_2019\n",
            "3시리즈_F30_2013_2018\t\t     더_뉴_QM6_2020_2023\n",
            "3시리즈_G20_2019_2022\t\t     뉴_QM6_2021_2023\n",
            "3시리즈_G20_2023_2025\t\t     더_뉴_QM6_2024_2025\n",
            "3시리즈_GT_F34_2014_2021\t     QX60_2016_2018\n",
            "디스커버리_4_2010_2016\t\t     뉴쏘렌토_R_2013_2014\n",
            "레인지로버_4세대_2014_2017\t     더_뉴스포티지R_2014_2016\n",
            "몬데오_4세대_2015_2020\t\t     RAV4_2016_2018\n",
            "프리우스_4세대_2016_2018\t     RAV4_5세대_2019_2024\n",
            "스포티지_4세대_2016_2018\t     S60_3세대_2020_2024\n",
            "레인지로버_4세대_2018_2022\t     S90_2017_2020\n",
            "프리우스_4세대_2019_2022\t     S90_2021_2025\n",
            "카니발_4세대_2021\t\t     SM3_네오_2015_2019\n",
            "쏘렌토_4세대_2021_2023\t\t     뉴_SM5_임프레션_2008_2010\n",
            "시에나_4세대_2021_2024\t\t     뉴_SM5_플래티넘_2013_2014\n",
            "카니발_4세대_2022_2023\t\t     SM5_노바_2015_2019\n",
            "더_뉴_카니발_4세대_2024_2025\t     SM6_2016_2020\n",
            "더_뉴_쏘렌토_4세대_2024_2025\t     더_뉴_SM6_2021_2024\n",
            "라브4_4세대_2013_2018\t\t     SM7_뉴아트_2008_2011\n",
            "라브4_5세대_2019_2024\t\t     SM7_노바_2015_2019\n",
            "4시리즈_F32_2014_2020\t\t     S_클래스_W221_2006_2013\n",
            "4시리즈_G22_2021_2023\t\t     S_클래스_W222_2014_2020\n",
            "4시리즈_G22_2024_2025\t\t     S_클래스_W223_2021_2025\n",
            "5008_2세대_2018_2019\t\t     코나_SX2_2023_2025\n",
            "5008_2세대_2021_2024\t\t     그랜저TG_2007_2008\n",
            "디스커버리_5_2017_2020\t\t     올_뉴_투싼_TL_2016_2018\n",
            "에스컬레이드_5세대_2021_2024\t     올_뉴_투싼_TL_2019_2020\n",
            "아이오닉5_2022_2023\t\t     싼타페_TM_2019_2020\n",
            "디스커버리_5_2022_2024\t\t     UX250h_2019_2024\n",
            "스포티지_5세대_2022_2024\t     V40_2015_2018\n",
            "레인지로버_5세대_2023_2024\t     V60_크로스컨트리_2세대_2020_2025\n",
            "5시리즈_F10_2010_2016\t\t     V90_크로스컨트리_2018_2024\n",
            "5시리즈_G30_2017_2023\t\t     뉴_체어맨_W_2012_2016\n",
            "5시리즈_G60_2024_2025\t\t     그랜드_체로키_WL_2021_2023\n",
            "5시리즈_GT_F07_2010_2017\t     X1_F48_2016_2019\n",
            "익스플로러_6세대_2020_2025\t     X1_F48_2020_2022\n",
            "아이오닉6_2023_2025\t\t     X1_U11_2023_2024\n",
            "6시리즈_F12_2011_2018\t\t     X2_F39_2018_2023\n",
            "6시리즈_GT_G32_2018_2020\t     X3_G01_2018_2021\n",
            "6시리즈_GT_G32_2021_2024\t     X3_G01_2022_2024\n",
            "박스터_718_2017_2024\t\t     X4_F26_2015_2018\n",
            "718_카이맨_2017_2024\t\t     X4_G02_2019_2021\n",
            "718_박스터_2017_2024\t\t     X4_G02_2022_2025\n",
            "골프_7세대_2013_2016\t\t     X5_F15_2014_2018\n",
            "7시리즈_F01_2009_2015\t\t     X5_G05_2019_2023\n",
            "7시리즈_G11_2016_2018\t\t     X5_G05_2024_2025\n",
            "7시리즈_G11_2019_2022\t\t     X6_F16_2015_2019\n",
            "7시리즈_G70_2023_2025\t\t     X6_G06_2020_2023\n",
            "8시리즈_G15_2020_2024\t\t     X6_G06_2024_2025\n",
            "911_2003_2019\t\t\t     X7_G07_2019_2022\n",
            "911_992_2020_2024\t\t     X7_G07_2023_2025\n",
            "파나메라_971_2017_2023\t\t     XC40_2019_2022\n",
            "A4_B9_2016_2019\t\t\t     XC60_2세대_2018_2021\n",
            "A4_B9_2020_2024\t\t\t     XC60_2세대_2022_2025\n",
            "A5_F5_2019_2024\t\t\t     XC90_2세대_2017_2019\n",
            "뉴_A6_2012_2014\t\t\t     XC90_2세대_2020_2025\n",
            "뉴_A6_2015_2018\t\t\t     XE_2016_2019\n",
            "A6_C8_2019_2025\t\t\t     XF_X260_2016_2020\n",
            "A7_2012_2016\t\t\t     XJ_8세대_2010_2019\n",
            "A7_4K_2020_2024\t\t\t     XM3_2020_2023\n",
            "A8_D5_2018_2023\t\t\t     XM3_2024\n",
            "아반떼_AD_2016_2018\t\t     캠리_XV70_2018_2024\n",
            "더_뉴_아반떼_AD_2019_2020\t     모델_Y_2021_2025\n",
            "All_New_XJ_2016_2019\t\t     YF쏘나타_2009_2012\n",
            "AMG_GT_2016_2024\t\t     YF쏘나타_하이브리드_2011_2015\n",
            "A_클래스_W176_2015_2018\t\t     Z4_G29_2019_2025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import torch\n",
        "import timm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import StratifiedKFold\n"
      ],
      "metadata": {
        "id": "eyLBp4j3i6F1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train 경로\n",
        "train_dir = '/content/train'\n",
        "\n",
        "# test 경로\n",
        "test_dir = '/content/test'\n",
        "\n",
        "# sample_submission (추론 때 사용)\n",
        "sample_submission_path = '/content/sample_submission.csv'\n"
      ],
      "metadata": {
        "id": "Ybtf0x5si9CE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "def show_memory_status():\n",
        "    allocated = torch.cuda.memory_allocated() / (1024 ** 2)  # MB 단위\n",
        "    reserved = torch.cuda.memory_reserved() / (1024 ** 2)    # MB 단위\n",
        "    print(f\"📊 현재 GPU 메모리 상태: Allocated = {allocated:.2f} MB | Reserved = {reserved:.2f} MB\")\n",
        "\n",
        "# 현재 CUDA 사용 가능 여부 확인\n",
        "if torch.cuda.is_available():\n",
        "    print(\"🔍 초기화 전 GPU 메모리 상태:\")\n",
        "    show_memory_status()\n",
        "\n",
        "    # GPU 캐시 및 메모리 초기화\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.ipc_collect()\n",
        "    gc.collect()\n",
        "\n",
        "    print(\"\\n🧹 GPU 메모리 초기화 완료\")\n",
        "    print(\"🔍 초기화 후 GPU 메모리 상태:\")\n",
        "    show_memory_status()\n",
        "else:\n",
        "    print(\"❌ CUDA 사용 불가\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxuQLgBAmkIm",
        "outputId": "dc9edaac-5ac2-4a40-ad4d-c0a7459b7271"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 초기화 전 GPU 메모리 상태:\n",
            "📊 현재 GPU 메모리 상태: Allocated = 0.00 MB | Reserved = 0.00 MB\n",
            "\n",
            "🧹 GPU 메모리 초기화 완료\n",
            "🔍 초기화 후 GPU 메모리 상태:\n",
            "📊 현재 GPU 메모리 상태: Allocated = 0.00 MB | Reserved = 0.00 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "class CarImageDataset(Dataset):\n",
        "    def __init__(self, file_list, class_to_idx, transform=None, use_aspect=False, use_color=False):\n",
        "        self.file_list = file_list\n",
        "        self.class_to_idx = class_to_idx\n",
        "        self.transform = transform\n",
        "        self.use_aspect = use_aspect\n",
        "        self.use_color = use_color\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.file_list[idx]\n",
        "\n",
        "        # 🚗 Step 1: Load Image (RGB 고정)\n",
        "        image_pil = Image.open(path).convert(\"RGB\")\n",
        "\n",
        "        # 🚗 Step 2: Calculate Features if needed\n",
        "        width, height = image_pil.size\n",
        "        aspect_ratio = np.array([width / height], dtype=np.float32)\n",
        "\n",
        "        image_cv2 = cv2.cvtColor(np.array(image_pil), cv2.COLOR_RGB2BGR)\n",
        "        color_mean = image_cv2.mean(axis=(0, 1))\n",
        "        color_mean = color_mean[::-1]\n",
        "        color_mean = np.array(color_mean / 255.0, dtype=np.float32)\n",
        "\n",
        "        # 🚗 Step 3: Apply Transform\n",
        "        if self.transform:\n",
        "            image = self.transform(image_pil)\n",
        "        else:\n",
        "            image = transforms.ToTensor()(image_pil)  # fallback\n",
        "\n",
        "        # 🚗 Step 4: Extract label\n",
        "        class_name = os.path.basename(os.path.dirname(path))\n",
        "        label = self.class_to_idx[class_name]\n",
        "\n",
        "        # 🚗 Step 5: Return according to mode\n",
        "        if self.use_aspect and self.use_color:\n",
        "            return image, torch.tensor(aspect_ratio), torch.tensor(color_mean), label\n",
        "        elif self.use_aspect:\n",
        "            return image, torch.tensor(aspect_ratio), label\n",
        "        elif self.use_color:\n",
        "            return image, torch.tensor(color_mean), label\n",
        "        else:\n",
        "            return image, label\n"
      ],
      "metadata": {
        "id": "rtI0lYOuJZt5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import glob\n",
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# ✅ 전체 JPG 파일 불러오기 (Train)\n",
        "file_list = glob.glob('/content/train/*/*.jpg')\n",
        "\n",
        "# ✅ 클래스명 추출 (폴더명)\n",
        "def extract_class_name_jpg(path):\n",
        "    return os.path.basename(os.path.dirname(path))\n",
        "\n",
        "class_names = sorted(set(extract_class_name_jpg(f) for f in file_list))\n",
        "class_to_idx = {cls: idx for idx, cls in enumerate(class_names)}\n",
        "\n",
        "print(f\"✅ 클래스 수: {len(class_to_idx)}\")  # 396개 나와야 정상\n",
        "\n",
        "# ✅ 라벨 생성\n",
        "labels = [class_to_idx[extract_class_name_jpg(f)] for f in file_list]\n",
        "\n",
        "# ✅ Train/Val Split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_files, val_files = train_test_split(file_list, test_size=0.1, stratify=labels, random_state=42)\n",
        "\n",
        "# ✅ Transform 정의\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((384, 384)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((384, 384)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# ✅ 확장형 Dataset 클래스 (앞서 만든 버전 사용!)\n",
        "class CarImageDataset(Dataset):\n",
        "    def __init__(self, file_list, class_to_idx, transform=None, use_aspect=False, use_color=False):\n",
        "        self.file_list = file_list\n",
        "        self.class_to_idx = class_to_idx\n",
        "        self.transform = transform\n",
        "        self.use_aspect = use_aspect\n",
        "        self.use_color = use_color\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.file_list[idx]\n",
        "\n",
        "        # 🚗 Load Image (RGB 고정)\n",
        "        image_pil = Image.open(path).convert(\"RGB\")\n",
        "\n",
        "        # 🚗 Feature: Aspect Ratio\n",
        "        width, height = image_pil.size\n",
        "        aspect_ratio = torch.tensor([width / height], dtype=torch.float32)\n",
        "\n",
        "        # 🚗 Feature: Dominant Color (mean RGB)\n",
        "        image_np = np.array(image_pil)\n",
        "        color_mean = image_np.mean(axis=(0, 1)) / 255.0  # Normalize to 0~1\n",
        "        color_mean = torch.tensor(color_mean, dtype=torch.float32)\n",
        "\n",
        "        # 🚗 Transform\n",
        "        if self.transform:\n",
        "            image = self.transform(image_pil)\n",
        "        else:\n",
        "            image = transforms.ToTensor()(image_pil)\n",
        "\n",
        "        # 🚗 Label\n",
        "        class_name = extract_class_name_jpg(path)\n",
        "        label = self.class_to_idx[class_name]\n",
        "\n",
        "        # 🚗 Return mode\n",
        "        if self.use_aspect and self.use_color:\n",
        "            return image, aspect_ratio, color_mean, label\n",
        "        elif self.use_aspect:\n",
        "            return image, aspect_ratio, label\n",
        "        elif self.use_color:\n",
        "            return image, color_mean, label\n",
        "        else:\n",
        "            return image, label\n",
        "\n",
        "# ✅ 실험 설정 (Base / Aspect / Color / Aspect+Color)\n",
        "USE_ASPECT = False    # 실험 A → Base / True → 실험 B/D\n",
        "USE_COLOR = False     # 실험 A → Base / True → 실험 C/D\n",
        "\n",
        "# ✅ Dataset 정의\n",
        "train_dataset = CarImageDataset(train_files, class_to_idx, train_transform, use_aspect=USE_ASPECT, use_color=USE_COLOR)\n",
        "val_dataset = CarImageDataset(val_files, class_to_idx, val_transform, use_aspect=USE_ASPECT, use_color=USE_COLOR)\n",
        "\n",
        "# ✅ DataLoader 정의\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True, persistent_workers=True, prefetch_factor=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True, persistent_workers=True, prefetch_factor=2)\n"
      ],
      "metadata": {
        "id": "u2DB9tCSJcmI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18f68ca7-467c-4612-faf9-4b2c02a2cd73"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 클래스 수: 396\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import timm\n",
        "import torch\n",
        "\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self, use_aspect, use_color, num_classes):\n",
        "        super(CustomModel, self).__init__()\n",
        "\n",
        "        self.use_aspect = use_aspect\n",
        "        self.use_color = use_color\n",
        "\n",
        "        # ✅ EfficientNet-B5 backbone\n",
        "        self.backbone = timm.create_model('efficientnet_b5', pretrained=True, num_classes=0)  # feature extractor\n",
        "        backbone_out_features = self.backbone.num_features\n",
        "\n",
        "        # ✅ Meta feature dimension 계산\n",
        "        meta_features_dim = 0\n",
        "        if self.use_aspect:\n",
        "            meta_features_dim += 1  # aspect ratio 1개\n",
        "        if self.use_color:\n",
        "            meta_features_dim += 3  # color_mean (R, G, B) 3개\n",
        "\n",
        "        # ✅ Classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(backbone_out_features + meta_features_dim, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, image, aspect_ratio=None, color_mean=None):\n",
        "        # ✅ EfficientNet feature\n",
        "        x = self.backbone(image)\n",
        "\n",
        "        # ✅ Meta features concat\n",
        "        aux_list = []\n",
        "\n",
        "        if self.use_aspect:\n",
        "            aux_list.append(aspect_ratio)\n",
        "\n",
        "        if self.use_color:\n",
        "            aux_list.append(color_mean)\n",
        "\n",
        "        if aux_list:\n",
        "            aux_features = torch.cat(aux_list, dim=1)\n",
        "            x = torch.cat([x, aux_features], dim=1)\n",
        "\n",
        "        # ✅ Final classifier\n",
        "        out = self.classifier(x)\n",
        "        return out\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = timm.create_model('efficientnet_b5', pretrained=True, num_classes=396)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "tNalQaGrKS5u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180,
          "referenced_widgets": [
            "54edd91cb3234752a4b6e94932efb7d3",
            "196f46de6b1e46da95b92448780b8e1c",
            "3e898f0791484fd9aaac886828f9168f",
            "0c03fc4a4f2c4438a0b8341112a31b0c",
            "81fe852edc7f4bf994049a40fdfe5bd8",
            "05092434c3f24220a4a59fa45b13c819",
            "35314ad301f34e11879a9110007009c6",
            "83cad76172624e458c14f56becdf1d3b",
            "fcc10c44e092413c80560d8d453cfba7",
            "82be077dbea94da883045ba0eae50070",
            "3cb6c7a8f0214f3b9d92a686307172bc"
          ]
        },
        "outputId": "8a009707-7bde-4796-c415-b48f2ddcfec0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/122M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "54edd91cb3234752a4b6e94932efb7d3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# AdamW + weight_decay 추가 추천 (EffNet 계열에 많이 사용)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n"
      ],
      "metadata": {
        "id": "JNIgNO9AKcl1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "import copy\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import timm\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# ✅ device 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ✅ 전체 jpg 파일\n",
        "file_list = glob.glob('/content/train/*/*.jpg')\n",
        "\n",
        "# ✅ 클래스명 추출\n",
        "def extract_class_name_jpg(path):\n",
        "    return os.path.basename(os.path.dirname(path))\n",
        "\n",
        "class_names = sorted(set(extract_class_name_jpg(f) for f in file_list))\n",
        "class_to_idx = {cls: idx for idx, cls in enumerate(class_names)}\n",
        "\n",
        "print(f\"✅ 클래스 수: {len(class_to_idx)}\")\n",
        "\n",
        "# ✅ 라벨 생성\n",
        "labels = [class_to_idx[extract_class_name_jpg(f)] for f in file_list]\n",
        "\n",
        "# ✅ Aspect Ratio Feature 함수\n",
        "def compute_aspect_ratio(path):\n",
        "    with Image.open(path) as img:\n",
        "        w, h = img.size\n",
        "        return w / h\n",
        "\n",
        "# ✅ Dominant Color Feature 함수 (간단한 RGB 평균 사용)\n",
        "def compute_dominant_color(path):\n",
        "    with Image.open(path).convert(\"RGB\") as img:\n",
        "        img = img.resize((16, 16))  # 작은 크기로 줄여서 평균 계산\n",
        "        np_img = np.array(img) / 255.0\n",
        "        mean_color = np_img.mean(axis=(0, 1))  # R, G, B 평균\n",
        "        return mean_color  # (3,)\n",
        "\n",
        "# ✅ Dataset 클래스 정의 (JPG용)\n",
        "class CarJPGDataset(Dataset):\n",
        "    def __init__(self, file_list, class_to_idx, transform=None, use_aspect=False, use_color=False):\n",
        "        self.file_list = file_list\n",
        "        self.class_to_idx = class_to_idx\n",
        "        self.transform = transform\n",
        "        self.use_aspect = use_aspect\n",
        "        self.use_color = use_color\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.file_list[idx]\n",
        "        image = Image.open(path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        class_name = extract_class_name_jpg(path)\n",
        "        label = self.class_to_idx[class_name]\n",
        "\n",
        "        meta_features = []\n",
        "\n",
        "        # ✅ Aspect Ratio 추가\n",
        "        if self.use_aspect:\n",
        "            ar = compute_aspect_ratio(path)\n",
        "            meta_features.append(ar)\n",
        "\n",
        "        # ✅ Dominant Color 추가\n",
        "        if self.use_color:\n",
        "            color = compute_dominant_color(path)  # (3,)\n",
        "            meta_features.extend(color.tolist())\n",
        "\n",
        "        # if meta_features:\n",
        "        #     meta_features = torch.tensor(meta_features, dtype=torch.float32)\n",
        "        #     return image, meta_features, label\n",
        "        # else:\n",
        "        #     return image, label\n",
        "        if meta_features:\n",
        "            meta_features = torch.tensor(meta_features, dtype=torch.float32)\n",
        "        else:\n",
        "            meta_features = torch.zeros(3, dtype=torch.float32)  # <= Dummy tensor (0,0,0)\n",
        "\n",
        "        return image, meta_features, label\n",
        "# ✅ transform 정의\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((384, 384)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((384, 384)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "# ✅ StratifiedKFold 정의\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# ✅ 전략 설정 (전략 C)\n",
        "EXPERIMENT = \"C\"  # A / B / C / D\n",
        "\n",
        "use_aspect = False\n",
        "use_color = True\n",
        "\n",
        "print(f\"\\n🚀 실험 설정: {EXPERIMENT} (Aspect={use_aspect}, Color={use_color})\\n\")\n",
        "\n",
        "✅ 5-Fold 루프 시작\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(file_list, labels)):\n",
        "    print(f\"\\n==============================\")\n",
        "    print(f\"🔁 Fold {fold + 1} / 5\")\n",
        "    print(f\"==============================\\n\")\n",
        "\n",
        "    # ✅ Fold별 split\n",
        "    train_files = [file_list[i] for i in train_idx]\n",
        "    val_files = [file_list[i] for i in val_idx]\n",
        "\n",
        "    # ✅ Fold별 Dataset & DataLoader\n",
        "    train_dataset = CarJPGDataset(train_files, class_to_idx, train_transform, use_aspect, use_color)\n",
        "    val_dataset = CarJPGDataset(val_files, class_to_idx, val_transform, use_aspect, use_color)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True, persistent_workers=True, prefetch_factor=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True, persistent_workers=True, prefetch_factor=2)\n",
        "\n",
        "    # ✅ Fold별 model / criterion / optimizer 초기화\n",
        "    model = CustomModel(use_aspect=use_aspect, use_color=use_color, num_classes=396)\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "\n",
        "    # ✅ EarlyStopping 변수 초기화\n",
        "    best_val_loss = float('inf')\n",
        "    patience = 3\n",
        "    patience_counter = 0\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    # ✅ Epoch 루프\n",
        "    for epoch in range(1, 31):\n",
        "        print(f\"\\n📌 Fold {fold+1} | Epoch {epoch}\")\n",
        "\n",
        "        # === 학습 ===\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "\n",
        "        loop = tqdm(train_loader, desc=f\"Train Fold {fold+1}\", leave=False)\n",
        "        for batch in loop:\n",
        "            # ✅ 전략 C (USE_COLOR=True)\n",
        "            if use_aspect and use_color:\n",
        "                X, meta_aspect, meta_color, y = batch\n",
        "                X, meta_aspect, meta_color, y = X.to(device), meta_aspect.to(device), meta_color.to(device), y.to(device)\n",
        "                outputs = model(X, aspect_ratio=meta_aspect, color_mean=meta_color)\n",
        "            elif use_aspect:\n",
        "                X, meta_aspect, y = batch\n",
        "                X, meta_aspect, y = X.to(device), meta_aspect.to(device), y.to(device)\n",
        "                outputs = model(X, aspect_ratio=meta_aspect)\n",
        "            elif use_color:\n",
        "                X, meta_color, y = batch\n",
        "                X, meta_color, y = X.to(device), meta_color.to(device), y.to(device)\n",
        "                outputs = model(X, color_mean=meta_color)\n",
        "            else:\n",
        "                X, y = batch\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                outputs = model(X)\n",
        "\n",
        "            loss = criterion(outputs, y)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * X.size(0)\n",
        "            train_correct += (outputs.argmax(1) == y).sum().item()\n",
        "            loop.set_postfix(loss=loss.item())\n",
        "\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "        train_acc = train_correct / len(train_loader.dataset)\n",
        "\n",
        "        # === 검증 ===\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "\n",
        "        val_loop = tqdm(val_loader, desc=f\"Valid Fold {fold+1}\", leave=False)\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loop:\n",
        "                if use_aspect and use_color:\n",
        "                    X, meta_aspect, meta_color, y = batch\n",
        "                    X, meta_aspect, meta_color, y = X.to(device), meta_aspect.to(device), meta_color.to(device), y.to(device)\n",
        "                    outputs = model(X, aspect_ratio=meta_aspect, color_mean=meta_color)\n",
        "                elif use_aspect:\n",
        "                    X, meta_aspect, y = batch\n",
        "                    X, meta_aspect, y = X.to(device), meta_aspect.to(device), y.to(device)\n",
        "                    outputs = model(X, aspect_ratio=meta_aspect)\n",
        "                elif use_color:\n",
        "                    X, meta_color, y = batch\n",
        "                    X, meta_color, y = X.to(device), meta_color.to(device), y.to(device)\n",
        "                    outputs = model(X, color_mean=meta_color)\n",
        "                else:\n",
        "                    X, y = batch\n",
        "                    X, y = X.to(device), y.to(device)\n",
        "                    outputs = model(X)\n",
        "\n",
        "                loss = criterion(outputs, y)\n",
        "                val_loss += loss.item() * X.size(0)\n",
        "                val_correct += (outputs.argmax(1) == y).sum().item()\n",
        "                val_loop.set_postfix(loss=loss.item())\n",
        "\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        val_acc = val_correct / len(val_loader.dataset)\n",
        "\n",
        "        # === 로그 출력 ===\n",
        "        print(f\"✅ Fold {fold+1} | Epoch {epoch} | Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f}\")\n",
        "        print(f\"✅ Fold {fold+1} | Epoch {epoch} | Val   Loss: {val_loss:.4f} | Acc: {val_acc:.4f}\")\n",
        "\n",
        "        # === EarlyStopping ===\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            save_path = f\"/content/drive/MyDrive/team_models/EffNetB5_{EXPERIMENT}_fold{fold+1}.pth\"\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print(f\"📦 Best model saved for Fold {fold+1}!\")\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"⚠️ EarlyStopping patience: {patience_counter}/{patience}\")\n",
        "            if patience_counter >= patience:\n",
        "                print(\"⛔ Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "    # ✅ Fold 끝나고 Best 모델 로드\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    print(f\"✅ Fold {fold+1} Best model loaded.\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCZJkMSSKeiG",
        "outputId": "077bb435-41e0-4b00-f3c0-4e2e72fd9e58"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 클래스 수: 396\n",
            "\n",
            "🚀 실험 설정: C (Aspect=False, Color=True)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import timm\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# ✅ 고정 경로\n",
        "TEST_DIR = \"/content/test\"\n",
        "SAMPLE_SUB_PATH = \"/content/sample_submission.csv\"\n",
        "NUM_CLASSES = 396\n",
        "\n",
        "# ✅ 샘플 제출 파일에서 클래스명 추출\n",
        "sample = pd.read_csv(SAMPLE_SUB_PATH)\n",
        "column_names = sample.columns.tolist()[1:]  # 'ID' 제외\n",
        "\n",
        "# ✅ Transform (JPG용)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((384, 384)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# ✅ Dominant Color 계산 함수\n",
        "def compute_dominant_color(path):\n",
        "    with Image.open(path).convert(\"RGB\") as img:\n",
        "        img = img.resize((16, 16))\n",
        "        np_img = np.array(img) / 255.0\n",
        "        mean_color = np_img.mean(axis=(0, 1))\n",
        "        return mean_color  # (3,)\n",
        "\n",
        "# ✅ 테스트용 Dataset (JPG용 + color_mean 포함)\n",
        "class TestJPGDatasetWithColor(Dataset):\n",
        "    def __init__(self, img_root, transform=None):\n",
        "        self.file_list = []\n",
        "        for file in os.listdir(img_root):\n",
        "            if file.endswith('.jpg'):\n",
        "                self.file_list.append(os.path.join(img_root, file))\n",
        "        self.file_list.sort()\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.file_list[idx]\n",
        "        image = Image.open(path).convert('RGB')\n",
        "\n",
        "        # color_mean 계산\n",
        "        color_mean = compute_dominant_color(path)\n",
        "        color_mean = torch.tensor(color_mean, dtype=torch.float32)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        fname = os.path.basename(path).replace(\".jpg\", \"\")\n",
        "        return image, color_mean, fname\n",
        "\n",
        "# ✅ DataLoader 고정\n",
        "test_dataset = TestJPGDatasetWithColor(TEST_DIR, transform)\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=64,\n",
        "    shuffle=False,\n",
        "    num_workers=6,\n",
        "    pin_memory=True,\n",
        "    prefetch_factor=4\n",
        ")\n",
        "\n",
        "# ✅ 디바이스 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ✅ 실험 리스트 (전략 C만!)\n",
        "exp_list = [\"C\"]\n",
        "\n",
        "# ✅ 비교 결과 저장용 (submission 합치기)\n",
        "all_submissions = []\n",
        "\n",
        "# ✅ 실험 루프 시작\n",
        "for exp_name in exp_list:\n",
        "    print(f\"\\n==============================\")\n",
        "    print(f\"🚀 START INFERENCE: EXPERIMENT {exp_name}\")\n",
        "    print(f\"==============================\\n\")\n",
        "\n",
        "    # ✅ 모델 경로 자동 생성\n",
        "    FOLD_MODEL_PATHS = [\n",
        "        f\"/content/drive/MyDrive/team_models/EffNetB5_{exp_name}_fold1.pth\",\n",
        "        f\"/content/drive/MyDrive/team_models/EffNetB5_{exp_name}_fold2.pth\",\n",
        "        f\"/content/drive/MyDrive/team_models/EffNetB5_{exp_name}_fold3.pth\",\n",
        "        f\"/content/drive/MyDrive/team_models/EffNetB5_{exp_name}_fold4.pth\",\n",
        "        f\"/content/drive/MyDrive/team_models/EffNetB5_{exp_name}_fold5.pth\",\n",
        "    ]\n",
        "\n",
        "    # ✅ 앙상블 결과 초기화\n",
        "    ensemble_outputs = []\n",
        "\n",
        "    # ✅ Fold 모델 순서대로 추론\n",
        "    for fold_idx, model_path in enumerate(FOLD_MODEL_PATHS):\n",
        "        print(f\"\\n🚀 Inference with Fold {fold_idx + 1} Model: {model_path}\")\n",
        "\n",
        "        # ✅ 반드시 CustomModel 로드 (전략 C)\n",
        "        model = CustomModel(use_aspect=False, use_color=True, num_classes=NUM_CLASSES)\n",
        "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "\n",
        "        # fold별 output 저장\n",
        "        fold_probs = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for imgs, color_mean, names in tqdm(test_loader, desc=f\"🔍 Fold {fold_idx + 1} Inference\"):\n",
        "                imgs = imgs.to(device)\n",
        "                color_mean = color_mean.to(device)\n",
        "\n",
        "                outputs = model(imgs, color_mean=color_mean)\n",
        "                probs = F.softmax(outputs, dim=1)\n",
        "                fold_probs.append(probs.cpu().numpy())\n",
        "\n",
        "        fold_probs = np.concatenate(fold_probs, axis=0)\n",
        "        ensemble_outputs.append(fold_probs)\n",
        "\n",
        "    # ✅ 앙상블 평균\n",
        "    ensemble_outputs = np.stack(ensemble_outputs, axis=0)  # (num_folds, num_samples, num_classes)\n",
        "    mean_outputs = np.mean(ensemble_outputs, axis=0)       # (num_samples, num_classes)\n",
        "\n",
        "    # ✅ 결과 저장\n",
        "    results = []\n",
        "    for idx, path in enumerate(test_dataset.file_list):\n",
        "        fname = os.path.basename(path).replace(\".jpg\", \"\")\n",
        "        row = {\"ID\": fname}\n",
        "        row.update({class_name: mean_outputs[idx, i] for i, class_name in enumerate(column_names)})\n",
        "        results.append(row)\n",
        "\n",
        "    submission_df = pd.DataFrame(results)\n",
        "    submission_df = submission_df[[\"ID\"] + column_names]\n",
        "\n",
        "    # ✅ 파일 저장\n",
        "    SAVE_SUBMISSION_PATH = f\"/content/drive/MyDrive/team_models/submission_fold5_ensemble_{exp_name}.csv\"\n",
        "    submission_df.to_csv(SAVE_SUBMISSION_PATH, index=False)\n",
        "\n",
        "    print(f\"\\n✅ 앙상블 서브미션 저장 완료: {SAVE_SUBMISSION_PATH}\")\n",
        "\n",
        "    # ✅ 비교용으로 all_submissions에 저장\n",
        "    submission_df[\"experiment\"] = exp_name\n",
        "    all_submissions.append(submission_df)\n",
        "\n",
        "# ✅ 최종 비교용 DataFrame 만들기\n",
        "final_compare_df = pd.concat(all_submissions, axis=0)\n",
        "compare_save_path = \"/content/drive/MyDrive/team_models/all_experiments_submission_compare.csv\"\n",
        "final_compare_df.to_csv(compare_save_path, index=False)\n",
        "\n",
        "print(f\"\\n🎉 모든 실험 완료! 비교용 CSV 저장됨: {compare_save_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k61x77gkNSDG",
        "outputId": "e3d29185-c78d-4b2d-8354-cd9a8440c765",
        "collapsed": true
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "🚀 START INFERENCE: EXPERIMENT C\n",
            "==============================\n",
            "\n",
            "\n",
            "🚀 Inference with Fold 1 Model: /content/drive/MyDrive/team_models/EffNetB5_C_fold1.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "🔍 Fold 1 Inference: 100%|██████████| 130/130 [00:25<00:00,  5.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Inference with Fold 2 Model: /content/drive/MyDrive/team_models/EffNetB5_C_fold2.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "🔍 Fold 2 Inference: 100%|██████████| 130/130 [00:25<00:00,  5.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Inference with Fold 3 Model: /content/drive/MyDrive/team_models/EffNetB5_C_fold3.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "🔍 Fold 3 Inference: 100%|██████████| 130/130 [00:25<00:00,  5.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Inference with Fold 4 Model: /content/drive/MyDrive/team_models/EffNetB5_C_fold4.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "🔍 Fold 4 Inference: 100%|██████████| 130/130 [00:25<00:00,  5.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Inference with Fold 5 Model: /content/drive/MyDrive/team_models/EffNetB5_C_fold5.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "🔍 Fold 5 Inference: 100%|██████████| 130/130 [00:25<00:00,  5.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ 앙상블 서브미션 저장 완료: /content/drive/MyDrive/team_models/submission_fold5_ensemble_C.csv\n",
            "\n",
            "🎉 모든 실험 완료! 비교용 CSV 저장됨: /content/drive/MyDrive/team_models/all_experiments_submission_compare.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# ✅ 전략 C 앙상블 결과물 경로\n",
        "submission_path = \"/content/drive/MyDrive/team_models/submission_fold5_ensemble_C.csv\"\n",
        "\n",
        "# ✅ 다운로드 실행\n",
        "files.download(submission_path)\n",
        "\n",
        "# ✅ 전체 비교 CSV 다운로드\n",
        "compare_path = \"/content/drive/MyDrive/team_models/all_experiments_submission_compare.csv\"\n",
        "\n",
        "files.download(compare_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "G8IEkAgaXKJ0",
        "outputId": "0f68870b-b257-4101-f234-5d05155b77c2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_39b6eca2-86af-43d9-bb2f-fa1aaae98037\", \"submission_fold5_ensemble_C.csv\", 44635315)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4c06eb93-ae61-46a9-8664-ff0d049d84c5\", \"all_experiments_submission_compare.csv\", 44651842)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.amp import GradScaler, autocast\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "# ✅ GPU 메모리 초기화\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# ✅ 1. Teacher 모델 준비\n",
        "teacher = CustomModel(use_aspect=False, use_color=True, num_classes=396)\n",
        "teacher.load_state_dict(torch.load(\"/content/drive/MyDrive/team_models/EffNetB5_C_fold1.pth\"))\n",
        "teacher = teacher.to(device)\n",
        "teacher.eval()\n",
        "\n",
        "# ❗ Freeze teacher\n",
        "for param in teacher.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# ✅ 2. Student 모델 준비\n",
        "student = CustomModel(use_aspect=False, use_color=True, num_classes=396)\n",
        "student = student.to(device)\n",
        "\n",
        "# ✅ 3. Optimizer, Criterion, Scaler\n",
        "T_init = 2.0  # 🔥 초기 Temperature\n",
        "T_min = 1.0   # 🔥 최소 Temperature\n",
        "T_decay = 0.95  # 🔥 T 줄이는 비율\n",
        "\n",
        "criterion = nn.KLDivLoss(reduction='batchmean')\n",
        "optimizer = torch.optim.AdamW(student.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)  # 🔥 50에포크 기준\n",
        "scaler = GradScaler()  # ✅ FP16 Mixed Precision\n",
        "\n",
        "# ✅ 4. Dataset & DataLoader\n",
        "train_dataset = CarJPGDataset(train_files, class_to_idx, train_transform, use_aspect, use_color)\n",
        "val_dataset = CarJPGDataset(val_files, class_to_idx, val_transform, use_aspect, use_color)  # ✅ Validation 추가\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4,\n",
        "                          pin_memory=True, persistent_workers=True, prefetch_factor=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4,\n",
        "                        pin_memory=True, persistent_workers=True, prefetch_factor=2)\n",
        "\n",
        "# ✅ 5. Stage 2 학습 (Pseudo-Label 기반 Soft Label 학습)\n",
        "num_epochs = 50\n",
        "patience = 5\n",
        "best_val_loss = float('inf')\n",
        "counter = 0\n",
        "T = T_init  # 초기 T\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    student.train()\n",
        "    total_loss = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    loop = tqdm(train_loader, desc=f\"Stage2 Epoch {epoch}\")\n",
        "    for images, meta_features, _ in loop:\n",
        "        images = images.to(device)\n",
        "        meta_features = meta_features.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pseudo_logits = teacher(images, color_mean=meta_features)\n",
        "            pseudo_soft_labels = F.softmax(pseudo_logits / T, dim=1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        with autocast():  # ✅ Mixed Precision\n",
        "            outputs = student(images, color_mean=meta_features)\n",
        "            student_log_probs = F.log_softmax(outputs / T, dim=1)\n",
        "            loss = criterion(student_log_probs, pseudo_soft_labels) * (T * T)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        torch.nn.utils.clip_grad_norm_(student.parameters(), max_norm=5.0)  # ✅ Gradient Clipping\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_loss += loss.item() * images.size(0)\n",
        "        total_samples += images.size(0)\n",
        "\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_train_loss = total_loss / total_samples\n",
        "    print(f\"📚 Stage2 Epoch {epoch}: Train Loss={avg_train_loss:.4f}\")\n",
        "\n",
        "    # 🔥 Temperature 스케줄링\n",
        "    T = max(T_min, T * T_decay)\n",
        "\n",
        "    # ✅ Validation\n",
        "    student.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, meta_features, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            meta_features = meta_features.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            with autocast():\n",
        "                outputs = student(images, color_mean=meta_features)\n",
        "                student_log_probs = F.log_softmax(outputs / T, dim=1)\n",
        "                pseudo_logits = teacher(images, color_mean=meta_features)\n",
        "                pseudo_soft_labels = F.softmax(pseudo_logits / T, dim=1)\n",
        "                loss = criterion(student_log_probs, pseudo_soft_labels) * (T * T)\n",
        "\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            val_correct += (preds == labels).sum().item()\n",
        "            val_total += labels.size(0)\n",
        "\n",
        "    avg_val_loss = val_loss / val_total\n",
        "    val_acc = val_correct / val_total\n",
        "    print(f\"📚 Validation Loss={avg_val_loss:.4f} | Validation Accuracy={val_acc:.4f}\")\n",
        "\n",
        "    # ✅ Early Stopping Check\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        counter = 0\n",
        "        torch.save(student.state_dict(), \"/content/drive/MyDrive/team_models/Student_stage2_fold1_best.pth\")\n",
        "        print(f\"✅ Best model saved at Epoch {epoch}!\")\n",
        "    else:\n",
        "        counter += 1\n",
        "        print(f\"⚠️ EarlyStopping patience: {counter}/{patience}\")\n",
        "        if counter >= patience:\n",
        "            print(\"⛔ Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "    scheduler.step()  # ✅ Learning Rate Scheduler\n",
        "\n",
        "print(\"✅ 학습 완료!\")\n"
      ],
      "metadata": {
        "id": "QHn6wXuwqc4F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a0a0dda8-c131-4db1-cf1f-660c9d1331dd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-92c141f750cc>:34: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()  # ✅ FP16 Mixed Precision\n",
            "Stage2 Epoch 1:   0%|          | 0/466 [00:00<?, ?it/s]<ipython-input-19-92c141f750cc>:67: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():  # ✅ Mixed Precision\n",
            "Stage2 Epoch 1: 100%|██████████| 466/466 [03:43<00:00,  2.08it/s, loss=6.16]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📚 Stage2 Epoch 1: Train Loss=11.6124\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "<ipython-input-19-92c141f750cc>:100: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📚 Validation Loss=5.8136 | Validation Accuracy=0.7333\n",
            "✅ Best model saved at Epoch 1!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage2 Epoch 2: 100%|██████████| 466/466 [03:41<00:00,  2.10it/s, loss=1.73]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📚 Stage2 Epoch 2: Train Loss=3.2665\n",
            "📚 Validation Loss=1.0630 | Validation Accuracy=0.9083\n",
            "✅ Best model saved at Epoch 2!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage2 Epoch 3: 100%|██████████| 466/466 [03:41<00:00,  2.10it/s, loss=0.589]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📚 Stage2 Epoch 3: Train Loss=0.8881\n",
            "📚 Validation Loss=0.5562 | Validation Accuracy=0.9297\n",
            "✅ Best model saved at Epoch 3!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage2 Epoch 4: 100%|██████████| 466/466 [03:41<00:00,  2.10it/s, loss=0.624]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📚 Stage2 Epoch 4: Train Loss=0.5415\n",
            "📚 Validation Loss=0.3991 | Validation Accuracy=0.9390\n",
            "✅ Best model saved at Epoch 4!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage2 Epoch 5: 100%|██████████| 466/466 [03:41<00:00,  2.10it/s, loss=0.478]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📚 Stage2 Epoch 5: Train Loss=0.3643\n",
            "📚 Validation Loss=0.2510 | Validation Accuracy=0.9590\n",
            "✅ Best model saved at Epoch 5!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage2 Epoch 6: 100%|██████████| 466/466 [03:41<00:00,  2.10it/s, loss=0.224]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📚 Stage2 Epoch 6: Train Loss=0.2548\n",
            "📚 Validation Loss=0.2202 | Validation Accuracy=0.9575\n",
            "✅ Best model saved at Epoch 6!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage2 Epoch 7: 100%|██████████| 466/466 [03:41<00:00,  2.10it/s, loss=0.208]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📚 Stage2 Epoch 7: Train Loss=0.2091\n",
            "📚 Validation Loss=0.1924 | Validation Accuracy=0.9578\n",
            "✅ Best model saved at Epoch 7!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage2 Epoch 8: 100%|██████████| 466/466 [03:41<00:00,  2.10it/s, loss=0.11]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📚 Stage2 Epoch 8: Train Loss=0.1693\n",
            "📚 Validation Loss=0.1534 | Validation Accuracy=0.9590\n",
            "✅ Best model saved at Epoch 8!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage2 Epoch 9: 100%|██████████| 466/466 [03:41<00:00,  2.10it/s, loss=0.236]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📚 Stage2 Epoch 9: Train Loss=0.1344\n",
            "📚 Validation Loss=0.1146 | Validation Accuracy=0.9647\n",
            "✅ Best model saved at Epoch 9!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage2 Epoch 10: 100%|██████████| 466/466 [03:41<00:00,  2.10it/s, loss=0.122]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📚 Stage2 Epoch 10: Train Loss=0.1086\n",
            "📚 Validation Loss=0.1234 | Validation Accuracy=0.9605\n",
            "⚠️ EarlyStopping patience: 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage2 Epoch 11: 100%|██████████| 466/466 [03:41<00:00,  2.10it/s, loss=0.0387]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📚 Stage2 Epoch 11: Train Loss=0.1066\n",
            "📚 Validation Loss=0.1136 | Validation Accuracy=0.9599\n",
            "✅ Best model saved at Epoch 11!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage2 Epoch 12: 100%|██████████| 466/466 [03:41<00:00,  2.10it/s, loss=0.0556]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📚 Stage2 Epoch 12: Train Loss=0.0916\n",
            "📚 Validation Loss=0.1013 | Validation Accuracy=0.9614\n",
            "✅ Best model saved at Epoch 12!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage2 Epoch 13: 100%|██████████| 466/466 [03:41<00:00,  2.10it/s, loss=0.12]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📚 Stage2 Epoch 13: Train Loss=0.0816\n",
            "📚 Validation Loss=0.0971 | Validation Accuracy=0.9587\n",
            "✅ Best model saved at Epoch 13!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage2 Epoch 14: 100%|██████████| 466/466 [03:41<00:00,  2.10it/s, loss=0.0667]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📚 Stage2 Epoch 14: Train Loss=0.0678\n",
            "📚 Validation Loss=0.0726 | Validation Accuracy=0.9674\n",
            "✅ Best model saved at Epoch 14!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage2 Epoch 15: 100%|██████████| 466/466 [03:41<00:00,  2.10it/s, loss=0.0108]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📚 Stage2 Epoch 15: Train Loss=0.0469\n",
            "📚 Validation Loss=0.0705 | Validation Accuracy=0.9659\n",
            "✅ Best model saved at Epoch 15!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage2 Epoch 16: 100%|██████████| 466/466 [03:41<00:00,  2.10it/s, loss=0.0388]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📚 Stage2 Epoch 16: Train Loss=0.0453\n",
            "📚 Validation Loss=0.0652 | Validation Accuracy=0.9695\n",
            "✅ Best model saved at Epoch 16!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stage2 Epoch 17:  12%|█▏        | 56/466 [00:28<03:29,  1.96it/s, loss=0.0201]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-92c141f750cc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudent_log_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpseudo_soft_labels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5.0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# ✅ Gradient Clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# ✅ 경로 설정\n",
        "TEST_DIR = \"/content/test\"\n",
        "SAMPLE_SUB_PATH = \"/content/sample_submission.csv\"\n",
        "MODEL_PATH = \"/content/drive/MyDrive/team_models/Student_stage2_fold1_best.pth\"\n",
        "SUBMISSION_SAVE_PATH = \"/content/drive/MyDrive/team_models/submission_stage2_student.csv\"\n",
        "NUM_CLASSES = 396\n",
        "\n",
        "# ✅ 샘플 제출 파일에서 클래스명 추출\n",
        "sample = pd.read_csv(SAMPLE_SUB_PATH)\n",
        "column_names = sample.columns.tolist()[1:]  # 'ID' 제외\n",
        "\n",
        "# ✅ Transform 정의\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((384, 384)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# ✅ Dominant Color 계산 함수\n",
        "def compute_dominant_color(path):\n",
        "    with Image.open(path).convert(\"RGB\") as img:\n",
        "        img = img.resize((16, 16))\n",
        "        np_img = np.array(img) / 255.0\n",
        "        mean_color = np_img.mean(axis=(0, 1))\n",
        "        return mean_color  # (3,)\n",
        "\n",
        "# ✅ Test Dataset 정의\n",
        "class TestJPGDatasetWithColor(Dataset):\n",
        "    def __init__(self, img_root, transform=None):\n",
        "        self.file_list = []\n",
        "        for file in os.listdir(img_root):\n",
        "            if file.endswith('.jpg'):\n",
        "                self.file_list.append(os.path.join(img_root, file))\n",
        "        self.file_list.sort()\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.file_list[idx]\n",
        "        image = Image.open(path).convert('RGB')\n",
        "\n",
        "        # color_mean 계산\n",
        "        color_mean = compute_dominant_color(path)\n",
        "        color_mean = torch.tensor(color_mean, dtype=torch.float32)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        fname = os.path.basename(path).replace(\".jpg\", \"\")\n",
        "        return image, color_mean, fname\n",
        "\n",
        "# ✅ DataLoader\n",
        "test_dataset = TestJPGDatasetWithColor(TEST_DIR, transform)\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=64,\n",
        "    shuffle=False,\n",
        "    num_workers=6,\n",
        "    pin_memory=True,\n",
        "    prefetch_factor=4\n",
        ")\n",
        "\n",
        "# ✅ 디바이스 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ✅ 모델 로드 (Student 모델)\n",
        "student = CustomModel(use_aspect=False, use_color=True, num_classes=NUM_CLASSES)\n",
        "student.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
        "student = student.to(device)\n",
        "student.eval()\n",
        "\n",
        "# ✅ 추론\n",
        "predictions = []\n",
        "file_names = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, color_mean, names in tqdm(test_loader, desc=\"🔍 Student Model Inference\"):\n",
        "        imgs = imgs.to(device)\n",
        "        color_mean = color_mean.to(device)\n",
        "\n",
        "        outputs = student(imgs, color_mean=color_mean)\n",
        "        probs = F.softmax(outputs, dim=1)  # 소프트맥스 확률\n",
        "\n",
        "        predictions.append(probs.cpu().numpy())\n",
        "        file_names.extend(names)\n",
        "\n",
        "predictions = np.concatenate(predictions, axis=0)\n",
        "\n",
        "# ✅ 결과 저장\n",
        "results = []\n",
        "for idx, fname in enumerate(file_names):\n",
        "    row = {\"ID\": fname}\n",
        "    row.update({class_name: predictions[idx, i] for i, class_name in enumerate(column_names)})\n",
        "    results.append(row)\n",
        "\n",
        "submission_df = pd.DataFrame(results)\n",
        "submission_df = submission_df[[\"ID\"] + column_names]\n",
        "\n",
        "# ✅ CSV로 저장\n",
        "submission_df.to_csv(SUBMISSION_SAVE_PATH, index=False, encoding = 'utf-8-sig')\n",
        "print(f\"\\n✅ Submission 저장 완료: {SUBMISSION_SAVE_PATH}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BlGU3jTGlFa",
        "outputId": "563a7235-aabb-4a69-ebe2-dfe6a1b30fe2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "🔍 Student Model Inference: 100%|██████████| 130/130 [00:25<00:00,  5.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Submission 저장 완료: /content/drive/MyDrive/team_models/submission_stage2_student.csv\n"
          ]
        }
      ]
    }
  ]
}