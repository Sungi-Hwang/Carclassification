{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "1cRA_Nj6a-Yiu_yPR_1nV2aE1xx1yNi2N",
      "authorship_tag": "ABX9TyP2OM8wiSNx9AFYBW4jDNy9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sungi-Hwang/Carclassification/blob/main/Eff8_HSG_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc, torch, sys\n",
        "import os\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "# 1) ëª¨ë¸Â·í…ì„œÂ·ë°ì´í„° ì§€ìš°ê¸°\n",
        "# del model, xb, yb, logits, loss   # ë‚¨ì•„ ìˆì„ ë³€ìˆ˜ ì „ë¶€!\n",
        "gc.collect()\n",
        "\n",
        "# 2) CUDA ìºì‹œ ë¹„ìš°ê¸° (reserved í•´ì œ)\n",
        "torch.cuda.empty_cache()          # <â”€ ëŒ€ë¶€ë¶„ ì´ê±¸ë¡œ í•´ê²°\n",
        "\n",
        "# 3) í•„ìš”í•˜ë©´ ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™”ê¹Œì§€\n",
        "torch.cuda.ipc_collect()          # (ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ê°€ í• ë‹¹í•œ ë©”ëª¨ë¦¬ê¹Œì§€ íšŒìˆ˜)\n",
        "\n",
        "# 4) ë‚¨ì€ ê²Œ ìˆë‚˜ í™•ì¸\n",
        "print(torch.cuda.memory_allocated()/1e9,\n",
        "      torch.cuda.memory_reserved()/1e9, 'GB')\n"
      ],
      "metadata": {
        "id": "qg7auaiPa-g1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c4ea1f8-4f18-4eac-c3ae-78d107be36be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.734860288 6.07125504 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # í•œ ë²ˆë§Œ ì‹¤í–‰\n",
        "# !unzip -oq \"/content/drive/MyDrive/Colab Notebooks/open.zip\" \\\n",
        "#        -d \"/content/drive/MyDrive/Dacon/\"\n"
      ],
      "metadata": {
        "id": "0j2ZFxTS227Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1. IMPORT & ê¸°ë³¸ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "import os, random, numpy as np, torch, gc\n",
        "import torch.nn as nn, torch.nn.functional as F\n",
        "from torch.amp import autocast, GradScaler\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torchvision import transforms\n",
        "from torchvision.models import efficientnet_v2_m, EfficientNet_V2_M_Weights\n",
        "from pathlib import Path\n",
        "from PIL import Image, ImageFile; ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import log_loss\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32      = True\n",
        "AMP_DTYPE = torch.bfloat16\n",
        "# â— ëª¨ë“  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ í•œ ê³³ì—\n",
        "CFG = {\n",
        "    \"EPOCHS\"      : 15,\n",
        "    \"BATCH_SIZE\"  : 64,        # OOM ë‚˜ë©´ ë” â†“\n",
        "    \"LR\"          : 1e-4,\n",
        "    \"WEIGHT_DECAY\": 1e-4,\n",
        "}\n",
        "\n",
        "# â— FP16 ì´ ì•„ë‹Œ ê²½ìš°ì—” GradScaler íš¨ë ¥ ì—†ìŒ â†’ ìë™ ë¹„í™œì„±\n",
        "scaler = GradScaler(enabled = (AMP_DTYPE is torch.float16))"
      ],
      "metadata": {
        "id": "1IedKvjnLdaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.getcwd())"
      ],
      "metadata": {
        "id": "gunh9vkzk4Ep",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37cd2792-b503-478f-f363-83b846218955"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2. ë¼ë²¨ ì •ê·œí™” ë„ìš°ë¯¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "from collections import defaultdict\n",
        "\n",
        "alias_pairs = [\n",
        "    (\"K5_3ì„¸ëŒ€_í•˜ì´ë¸Œë¦¬ë“œ_2020_2022\", \"K5_í•˜ì´ë¸Œë¦¬ë“œ_3ì„¸ëŒ€_2020_2023\"),\n",
        "    (\"ë””_ì˜¬ë‰´ë‹ˆë¡œ_2022_2025\"      , \"ë””_ì˜¬_ë‰´_ë‹ˆë¡œ_2022_2025\"),\n",
        "    (\"718_ë°•ìŠ¤í„°_2017_2024\"       , \"ë°•ìŠ¤í„°_718_2017_2024\"),\n",
        "]\n",
        "alias = {b: a for a, b in alias_pairs} | {a: a for a, _ in alias_pairs}\n",
        "canon  = lambda lbl: alias.get(lbl, lbl)\n",
        "# ğŸ” ì—­ë§¤í•‘\n",
        "canon_to_originals = defaultdict(list)\n",
        "for a, b in alias_pairs:\n",
        "    canon_to_originals[canon(a)].append(a)\n",
        "    canon_to_originals[canon(b)].append(b)\n"
      ],
      "metadata": {
        "id": "C6Fu6e3pi9BR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, root, transform=None):\n",
        "        self.samples = []  # (image_path, canonical_class)\n",
        "        self.original_classes = []  # original label\n",
        "        self.canonical_classes = []  # canonical label\n",
        "\n",
        "        for p in sorted(Path(root).iterdir()):\n",
        "            if p.is_dir():\n",
        "                orig = p.name\n",
        "                cls = canon(orig)\n",
        "                self.original_classes.append(orig)\n",
        "                self.canonical_classes.append(cls)\n",
        "\n",
        "                for img in p.glob(\"*.jpg\"):\n",
        "                    self.samples.append((img, cls))\n",
        "\n",
        "        self.classes = sorted(set(self.canonical_classes))\n",
        "        self.class_to_idx = {c: i for i, c in enumerate(self.classes)}\n",
        "        self.idx_to_class = {i: c for c, i in self.class_to_idx.items()}\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, cls = self.samples[idx]\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, self.class_to_idx[cls]\n"
      ],
      "metadata": {
        "id": "0RJCL3H_BK9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4. í´ë˜ìŠ¤ ëª©ë¡ & Transform â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "sub = pd.read_csv(\"/content/drive/MyDrive/Dacon/sample_submission.csv\")\n",
        "CLASS_NAMES = [canon(c) for c in sub.columns[1:]]\n",
        "NUM_CLASSES = len(CLASS_NAMES)\n",
        "\n",
        "weights  = EfficientNet_V2_M_Weights.DEFAULT\n",
        "preset   = weights.transforms()\n",
        "IMG_SIZE = 480\n",
        "train_tf = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.7, 1.0), antialias=True),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    preset,\n",
        "])\n",
        "val_tf = transforms.Compose([\n",
        "    transforms.Resize(int(IMG_SIZE*1.15), antialias=True),\n",
        "    transforms.CenterCrop(IMG_SIZE),\n",
        "    preset,\n",
        "])"
      ],
      "metadata": {
        "id": "zdECrE8zZXnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 5. Dataset & DataLoader â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "root = \"/content/drive/MyDrive/Dacon/train\"\n",
        "ds_full   = CustomImageDataset(root, transform=train_tf)\n",
        "ds_full_v = CustomImageDataset(root, transform=val_tf)\n",
        "\n",
        "targets   = [lbl for _, lbl in ds_full.samples]\n",
        "tr_idx, v_idx = train_test_split(\n",
        "    np.arange(len(ds_full)),\n",
        "    test_size=0.2,\n",
        "    stratify=targets,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    Subset(ds_full, tr_idx),\n",
        "    batch_size = CFG[\"BATCH_SIZE\"],     # â— CFG ì‚¬ìš©\n",
        "    shuffle    = True,\n",
        "    num_workers=8,\n",
        "    pin_memory = True,\n",
        "    persistent_workers=True,\n",
        "    prefetch_factor=4,\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    Subset(ds_full_v, v_idx),\n",
        "    batch_size = CFG[\"BATCH_SIZE\"],     # â— CFG ì‚¬ìš©\n",
        "    shuffle    = False,\n",
        "    num_workers=4,\n",
        "    pin_memory = True,\n",
        "    persistent_workers=True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "6msh-TKiYieo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 6. ëª¨ë¸, Optim, Criterion â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "model = efficientnet_v2_m(weights=weights)\n",
        "in_f  = model.classifier[1].in_features\n",
        "model.classifier[1] = nn.Linear(in_f, NUM_CLASSES)\n",
        "\n",
        "model = model.to(device, dtype=AMP_DTYPE, memory_format=torch.channels_last)  # â—\n",
        "model = model\n",
        "crit = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "opt  = torch.optim.AdamW(\n",
        "          model.parameters(),\n",
        "          lr          = CFG[\"LR\"],          # â—\n",
        "          weight_decay= CFG[\"WEIGHT_DECAY\"],\n",
        "          fused=True\n",
        "      )\n"
      ],
      "metadata": {
        "id": "pVdQDCJBYv3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 7. Train / Val ë£¨í”„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "best = np.inf\n",
        "SAVE_PATH = \"/content/drive/MyDrive/best_model.pth\"\n",
        "for ep in range(CFG[\"EPOCHS\"]):\n",
        "    # ---- train ----\n",
        "    model.train(); tl = 0\n",
        "    for xb, yb in tqdm(train_loader, desc=f\"Ep{ep+1} Train\"):\n",
        "        xb = xb.to(device, memory_format=torch.channels_last, dtype = AMP_DTYPE)  # â—\n",
        "        yb = yb.to(device)\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        # with torch.no_grad():\n",
        "        #     print(\"âœ” yb.shape:\", yb.shape)\n",
        "        #     print(\"âœ” yb.dtype:\", yb.dtype)\n",
        "        #     print(\"âœ” yb min:\", yb.min().item(), \"max:\", yb.max().item())\n",
        "        #     dummy_x = torch.randn_like(xb, dtype=AMP_DTYPE)\n",
        "        #     dummy_out = model(dummy_x)\n",
        "        #     print(\"âœ” logits.shape[1] (num_classes):\", dummy_out.shape[1])\n",
        "        #     assert yb.dtype == torch.long\n",
        "        #     assert yb.max() < dummy_out.shape[1], \"â— yb ê°’ì´ í´ë˜ìŠ¤ ìˆ˜ë³´ë‹¤ í½ë‹ˆë‹¤!\"\n",
        "        with autocast(\"cuda\",dtype=AMP_DTYPE):             # â—\n",
        "            logits = model(xb)\n",
        "            loss   = crit(logits, yb)\n",
        "\n",
        "        if scaler.is_enabled():                             # â— FP16 ì „ìš© ê²½ë¡œ\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "        else:                                               # â— BF16 ê²½ë¡œ\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        tl += loss.item()\n",
        "\n",
        "    # ---- val ----\n",
        "    model.eval(); all_p, all_y = [], []; correct = total = 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in tqdm(val_loader, desc=f\"Ep{ep+1} Val\"):\n",
        "            xb = xb.to(device, memory_format=torch.channels_last, dtype=AMP_DTYPE)\n",
        "            yb = yb.to(device)\n",
        "            # with torch.no_grad():\n",
        "            #     print(\"âœ” yb.shape:\", yb.shape)ì „ì²´\n",
        "            #     print(\"âœ” yb.dtype:\", yb.dtype)\n",
        "            #     print(\"âœ” yb min:\", yb.min().item(), \"max:\", yb.max().item())\n",
        "            #     print(\"âœ” logits.shape[1] (num_classes):\", model(torch.randn_like(xb)).shape[1])\n",
        "            #     assert yb.dtype == torch.long\n",
        "            #     assert yb.max() < model(torch.randn_like(xb)).shape[1]\n",
        "            with autocast(\"cuda\", dtype=AMP_DTYPE):\n",
        "                out = model(xb)\n",
        "            prob = F.softmax(out, 1)\n",
        "            pred = prob.argmax(1)\n",
        "            correct += (pred == yb).sum().item(); total += yb.size(0)\n",
        "            all_p.append(prob.cpu().float().numpy())\n",
        "            all_y.append(yb.cpu().numpy())\n",
        "\n",
        "    logloss = log_loss(np.concatenate(all_y), np.concatenate(all_p))\n",
        "    acc     = 100 * correct / total\n",
        "    print(f\"Ep{ep+1:02d}  train_loss={tl/len(train_loader):.4f}  \"\n",
        "          f\"val_logloss={logloss:.4f}  acc={acc:.2f}%\")\n",
        "\n",
        "    if logloss < best:\n",
        "        best = logloss\n",
        "        torch.save(model.state_dict(), SAVE_PATH)\n",
        "        print(\"   âœ… best_model.pth saved (improved)\")"
      ],
      "metadata": {
        "id": "cknC4-7WYyj1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "270f345a-6749-4e1f-86a8-cb115a5e694f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep1 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 415/415 [03:57<00:00,  1.75it/s]\n",
            "Ep1 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:40<00:00,  2.55it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep01  train_loss=3.9627  val_logloss=1.7676  acc=71.26%\n",
            "   âœ… best_model.pth saved (improved)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep2 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 415/415 [03:56<00:00,  1.75it/s]\n",
            "Ep2 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:39<00:00,  2.62it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep02  train_loss=1.6318  val_logloss=0.8110  acc=84.17%\n",
            "   âœ… best_model.pth saved (improved)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep3 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 415/415 [03:57<00:00,  1.75it/s]\n",
            "Ep3 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:40<00:00,  2.60it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep03  train_loss=1.2899  val_logloss=0.5949  acc=87.30%\n",
            "   âœ… best_model.pth saved (improved)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep4 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 415/415 [03:57<00:00,  1.75it/s]\n",
            "Ep4 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:40<00:00,  2.60it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep04  train_loss=1.1753  val_logloss=0.5197  acc=89.00%\n",
            "   âœ… best_model.pth saved (improved)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep5 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 415/415 [03:56<00:00,  1.76it/s]\n",
            "Ep5 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:40<00:00,  2.59it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep05  train_loss=1.1267  val_logloss=0.4636  acc=90.19%\n",
            "   âœ… best_model.pth saved (improved)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep6 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 415/415 [03:57<00:00,  1.74it/s]\n",
            "Ep6 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:39<00:00,  2.62it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep06  train_loss=1.0920  val_logloss=0.4350  acc=90.89%\n",
            "   âœ… best_model.pth saved (improved)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep7 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 415/415 [03:56<00:00,  1.75it/s]\n",
            "Ep7 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:40<00:00,  2.60it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep07  train_loss=1.0726  val_logloss=0.4196  acc=91.04%\n",
            "   âœ… best_model.pth saved (improved)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep8 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 415/415 [03:56<00:00,  1.75it/s]\n",
            "Ep8 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:39<00:00,  2.61it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep08  train_loss=1.0540  val_logloss=0.4007  acc=91.45%\n",
            "   âœ… best_model.pth saved (improved)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep9 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 415/415 [03:57<00:00,  1.75it/s]\n",
            "Ep9 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:40<00:00,  2.57it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep09  train_loss=1.0405  val_logloss=0.3981  acc=91.57%\n",
            "   âœ… best_model.pth saved (improved)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep10 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 415/415 [03:57<00:00,  1.75it/s]\n",
            "Ep10 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:40<00:00,  2.58it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep10  train_loss=1.0300  val_logloss=0.3938  acc=91.87%\n",
            "   âœ… best_model.pth saved (improved)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep11 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 415/415 [03:56<00:00,  1.75it/s]\n",
            "Ep11 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:40<00:00,  2.56it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep11  train_loss=1.0255  val_logloss=0.3905  acc=92.21%\n",
            "   âœ… best_model.pth saved (improved)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep12 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 415/415 [03:56<00:00,  1.75it/s]\n",
            "Ep12 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:40<00:00,  2.60it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep12  train_loss=1.0186  val_logloss=0.4046  acc=91.97%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep13 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 415/415 [03:55<00:00,  1.76it/s]\n",
            "Ep13 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:40<00:00,  2.58it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep13  train_loss=1.0182  val_logloss=0.3774  acc=92.17%\n",
            "   âœ… best_model.pth saved (improved)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep14 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 415/415 [03:56<00:00,  1.75it/s]\n",
            "Ep14 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:39<00:00,  2.60it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep14  train_loss=1.0122  val_logloss=0.3771  acc=92.53%\n",
            "   âœ… best_model.pth saved (improved)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep15 Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 415/415 [03:56<00:00,  1.75it/s]\n",
            "Ep15 Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 104/104 [00:39<00:00,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep15  train_loss=1.0042  val_logloss=0.3783  acc=92.41%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}