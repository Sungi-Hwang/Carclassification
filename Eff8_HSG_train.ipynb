{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "1cRA_Nj6a-Yiu_yPR_1nV2aE1xx1yNi2N",
      "authorship_tag": "ABX9TyP2OM8wiSNx9AFYBW4jDNy9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sungi-Hwang/Carclassification/blob/main/Eff8_HSG_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc, torch, sys\n",
        "import os\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "# 1) 모델·텐서·데이터 지우기\n",
        "# del model, xb, yb, logits, loss   # 남아 있을 변수 전부!\n",
        "gc.collect()\n",
        "\n",
        "# 2) CUDA 캐시 비우기 (reserved 해제)\n",
        "torch.cuda.empty_cache()          # <─ 대부분 이걸로 해결\n",
        "\n",
        "# 3) 필요하면 컨텍스트 초기화까지\n",
        "torch.cuda.ipc_collect()          # (다른 프로세스가 할당한 메모리까지 회수)\n",
        "\n",
        "# 4) 남은 게 있나 확인\n",
        "print(torch.cuda.memory_allocated()/1e9,\n",
        "      torch.cuda.memory_reserved()/1e9, 'GB')\n"
      ],
      "metadata": {
        "id": "qg7auaiPa-g1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c4ea1f8-4f18-4eac-c3ae-78d107be36be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.734860288 6.07125504 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # 한 번만 실행\n",
        "# !unzip -oq \"/content/drive/MyDrive/Colab Notebooks/open.zip\" \\\n",
        "#        -d \"/content/drive/MyDrive/Dacon/\"\n"
      ],
      "metadata": {
        "id": "0j2ZFxTS227Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ───────────────────── 1. IMPORT & 기본 설정 ─────────────────────\n",
        "import os, random, numpy as np, torch, gc\n",
        "import torch.nn as nn, torch.nn.functional as F\n",
        "from torch.amp import autocast, GradScaler\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torchvision import transforms\n",
        "from torchvision.models import efficientnet_v2_m, EfficientNet_V2_M_Weights\n",
        "from pathlib import Path\n",
        "from PIL import Image, ImageFile; ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import log_loss\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32      = True\n",
        "AMP_DTYPE = torch.bfloat16\n",
        "# ● 모든 하이퍼파라미터를 한 곳에\n",
        "CFG = {\n",
        "    \"EPOCHS\"      : 15,\n",
        "    \"BATCH_SIZE\"  : 64,        # OOM 나면 더 ↓\n",
        "    \"LR\"          : 1e-4,\n",
        "    \"WEIGHT_DECAY\": 1e-4,\n",
        "}\n",
        "\n",
        "# ● FP16 이 아닌 경우엔 GradScaler 효력 없음 → 자동 비활성\n",
        "scaler = GradScaler(enabled = (AMP_DTYPE is torch.float16))"
      ],
      "metadata": {
        "id": "1IedKvjnLdaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.getcwd())"
      ],
      "metadata": {
        "id": "gunh9vkzk4Ep",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37cd2792-b503-478f-f363-83b846218955"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ───────────────────── 2. 라벨 정규화 도우미 ─────────────────────\n",
        "from collections import defaultdict\n",
        "\n",
        "alias_pairs = [\n",
        "    (\"K5_3세대_하이브리드_2020_2022\", \"K5_하이브리드_3세대_2020_2023\"),\n",
        "    (\"디_올뉴니로_2022_2025\"      , \"디_올_뉴_니로_2022_2025\"),\n",
        "    (\"718_박스터_2017_2024\"       , \"박스터_718_2017_2024\"),\n",
        "]\n",
        "alias = {b: a for a, b in alias_pairs} | {a: a for a, _ in alias_pairs}\n",
        "canon  = lambda lbl: alias.get(lbl, lbl)\n",
        "# 🔁 역매핑\n",
        "canon_to_originals = defaultdict(list)\n",
        "for a, b in alias_pairs:\n",
        "    canon_to_originals[canon(a)].append(a)\n",
        "    canon_to_originals[canon(b)].append(b)\n"
      ],
      "metadata": {
        "id": "C6Fu6e3pi9BR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, root, transform=None):\n",
        "        self.samples = []  # (image_path, canonical_class)\n",
        "        self.original_classes = []  # original label\n",
        "        self.canonical_classes = []  # canonical label\n",
        "\n",
        "        for p in sorted(Path(root).iterdir()):\n",
        "            if p.is_dir():\n",
        "                orig = p.name\n",
        "                cls = canon(orig)\n",
        "                self.original_classes.append(orig)\n",
        "                self.canonical_classes.append(cls)\n",
        "\n",
        "                for img in p.glob(\"*.jpg\"):\n",
        "                    self.samples.append((img, cls))\n",
        "\n",
        "        self.classes = sorted(set(self.canonical_classes))\n",
        "        self.class_to_idx = {c: i for i, c in enumerate(self.classes)}\n",
        "        self.idx_to_class = {i: c for c, i in self.class_to_idx.items()}\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, cls = self.samples[idx]\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, self.class_to_idx[cls]\n"
      ],
      "metadata": {
        "id": "0RJCL3H_BK9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ───────────────────── 4. 클래스 목록 & Transform ─────────────────\n",
        "sub = pd.read_csv(\"/content/drive/MyDrive/Dacon/sample_submission.csv\")\n",
        "CLASS_NAMES = [canon(c) for c in sub.columns[1:]]\n",
        "NUM_CLASSES = len(CLASS_NAMES)\n",
        "\n",
        "weights  = EfficientNet_V2_M_Weights.DEFAULT\n",
        "preset   = weights.transforms()\n",
        "IMG_SIZE = 480\n",
        "train_tf = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.7, 1.0), antialias=True),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    preset,\n",
        "])\n",
        "val_tf = transforms.Compose([\n",
        "    transforms.Resize(int(IMG_SIZE*1.15), antialias=True),\n",
        "    transforms.CenterCrop(IMG_SIZE),\n",
        "    preset,\n",
        "])"
      ],
      "metadata": {
        "id": "zdECrE8zZXnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ───────────────────── 5. Dataset & DataLoader ───────────────────\n",
        "root = \"/content/drive/MyDrive/Dacon/train\"\n",
        "ds_full   = CustomImageDataset(root, transform=train_tf)\n",
        "ds_full_v = CustomImageDataset(root, transform=val_tf)\n",
        "\n",
        "targets   = [lbl for _, lbl in ds_full.samples]\n",
        "tr_idx, v_idx = train_test_split(\n",
        "    np.arange(len(ds_full)),\n",
        "    test_size=0.2,\n",
        "    stratify=targets,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    Subset(ds_full, tr_idx),\n",
        "    batch_size = CFG[\"BATCH_SIZE\"],     # ● CFG 사용\n",
        "    shuffle    = True,\n",
        "    num_workers=8,\n",
        "    pin_memory = True,\n",
        "    persistent_workers=True,\n",
        "    prefetch_factor=4,\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    Subset(ds_full_v, v_idx),\n",
        "    batch_size = CFG[\"BATCH_SIZE\"],     # ● CFG 사용\n",
        "    shuffle    = False,\n",
        "    num_workers=4,\n",
        "    pin_memory = True,\n",
        "    persistent_workers=True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "6msh-TKiYieo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ───────────────────── 6. 모델, Optim, Criterion ─────────────────\n",
        "model = efficientnet_v2_m(weights=weights)\n",
        "in_f  = model.classifier[1].in_features\n",
        "model.classifier[1] = nn.Linear(in_f, NUM_CLASSES)\n",
        "\n",
        "model = model.to(device, dtype=AMP_DTYPE, memory_format=torch.channels_last)  # ●\n",
        "model = model\n",
        "crit = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "opt  = torch.optim.AdamW(\n",
        "          model.parameters(),\n",
        "          lr          = CFG[\"LR\"],          # ●\n",
        "          weight_decay= CFG[\"WEIGHT_DECAY\"],\n",
        "          fused=True\n",
        "      )\n"
      ],
      "metadata": {
        "id": "pVdQDCJBYv3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ───────────────────── 7. Train / Val 루프 ───────────────────────\n",
        "best = np.inf\n",
        "SAVE_PATH = \"/content/drive/MyDrive/best_model.pth\"\n",
        "for ep in range(CFG[\"EPOCHS\"]):\n",
        "    # ---- train ----\n",
        "    model.train(); tl = 0\n",
        "    for xb, yb in tqdm(train_loader, desc=f\"Ep{ep+1} Train\"):\n",
        "        xb = xb.to(device, memory_format=torch.channels_last, dtype = AMP_DTYPE)  # ●\n",
        "        yb = yb.to(device)\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        # with torch.no_grad():\n",
        "        #     print(\"✔ yb.shape:\", yb.shape)\n",
        "        #     print(\"✔ yb.dtype:\", yb.dtype)\n",
        "        #     print(\"✔ yb min:\", yb.min().item(), \"max:\", yb.max().item())\n",
        "        #     dummy_x = torch.randn_like(xb, dtype=AMP_DTYPE)\n",
        "        #     dummy_out = model(dummy_x)\n",
        "        #     print(\"✔ logits.shape[1] (num_classes):\", dummy_out.shape[1])\n",
        "        #     assert yb.dtype == torch.long\n",
        "        #     assert yb.max() < dummy_out.shape[1], \"❗ yb 값이 클래스 수보다 큽니다!\"\n",
        "        with autocast(\"cuda\",dtype=AMP_DTYPE):             # ●\n",
        "            logits = model(xb)\n",
        "            loss   = crit(logits, yb)\n",
        "\n",
        "        if scaler.is_enabled():                             # ● FP16 전용 경로\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(opt)\n",
        "            scaler.update()\n",
        "        else:                                               # ● BF16 경로\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        tl += loss.item()\n",
        "\n",
        "    # ---- val ----\n",
        "    model.eval(); all_p, all_y = [], []; correct = total = 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in tqdm(val_loader, desc=f\"Ep{ep+1} Val\"):\n",
        "            xb = xb.to(device, memory_format=torch.channels_last, dtype=AMP_DTYPE)\n",
        "            yb = yb.to(device)\n",
        "            # with torch.no_grad():\n",
        "            #     print(\"✔ yb.shape:\", yb.shape)전체\n",
        "            #     print(\"✔ yb.dtype:\", yb.dtype)\n",
        "            #     print(\"✔ yb min:\", yb.min().item(), \"max:\", yb.max().item())\n",
        "            #     print(\"✔ logits.shape[1] (num_classes):\", model(torch.randn_like(xb)).shape[1])\n",
        "            #     assert yb.dtype == torch.long\n",
        "            #     assert yb.max() < model(torch.randn_like(xb)).shape[1]\n",
        "            with autocast(\"cuda\", dtype=AMP_DTYPE):\n",
        "                out = model(xb)\n",
        "            prob = F.softmax(out, 1)\n",
        "            pred = prob.argmax(1)\n",
        "            correct += (pred == yb).sum().item(); total += yb.size(0)\n",
        "            all_p.append(prob.cpu().float().numpy())\n",
        "            all_y.append(yb.cpu().numpy())\n",
        "\n",
        "    logloss = log_loss(np.concatenate(all_y), np.concatenate(all_p))\n",
        "    acc     = 100 * correct / total\n",
        "    print(f\"Ep{ep+1:02d}  train_loss={tl/len(train_loader):.4f}  \"\n",
        "          f\"val_logloss={logloss:.4f}  acc={acc:.2f}%\")\n",
        "\n",
        "    if logloss < best:\n",
        "        best = logloss\n",
        "        torch.save(model.state_dict(), SAVE_PATH)\n",
        "        print(\"   ✅ best_model.pth saved (improved)\")"
      ],
      "metadata": {
        "id": "cknC4-7WYyj1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "270f345a-6749-4e1f-86a8-cb115a5e694f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep1 Train: 100%|██████████| 415/415 [03:57<00:00,  1.75it/s]\n",
            "Ep1 Val: 100%|██████████| 104/104 [00:40<00:00,  2.55it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep01  train_loss=3.9627  val_logloss=1.7676  acc=71.26%\n",
            "   ✅ best_model.pth saved (improved)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep2 Train: 100%|██████████| 415/415 [03:56<00:00,  1.75it/s]\n",
            "Ep2 Val: 100%|██████████| 104/104 [00:39<00:00,  2.62it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep02  train_loss=1.6318  val_logloss=0.8110  acc=84.17%\n",
            "   ✅ best_model.pth saved (improved)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep3 Train: 100%|██████████| 415/415 [03:57<00:00,  1.75it/s]\n",
            "Ep3 Val: 100%|██████████| 104/104 [00:40<00:00,  2.60it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep03  train_loss=1.2899  val_logloss=0.5949  acc=87.30%\n",
            "   ✅ best_model.pth saved (improved)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep4 Train: 100%|██████████| 415/415 [03:57<00:00,  1.75it/s]\n",
            "Ep4 Val: 100%|██████████| 104/104 [00:40<00:00,  2.60it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep04  train_loss=1.1753  val_logloss=0.5197  acc=89.00%\n",
            "   ✅ best_model.pth saved (improved)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep5 Train: 100%|██████████| 415/415 [03:56<00:00,  1.76it/s]\n",
            "Ep5 Val: 100%|██████████| 104/104 [00:40<00:00,  2.59it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep05  train_loss=1.1267  val_logloss=0.4636  acc=90.19%\n",
            "   ✅ best_model.pth saved (improved)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep6 Train: 100%|██████████| 415/415 [03:57<00:00,  1.74it/s]\n",
            "Ep6 Val: 100%|██████████| 104/104 [00:39<00:00,  2.62it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep06  train_loss=1.0920  val_logloss=0.4350  acc=90.89%\n",
            "   ✅ best_model.pth saved (improved)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep7 Train: 100%|██████████| 415/415 [03:56<00:00,  1.75it/s]\n",
            "Ep7 Val: 100%|██████████| 104/104 [00:40<00:00,  2.60it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep07  train_loss=1.0726  val_logloss=0.4196  acc=91.04%\n",
            "   ✅ best_model.pth saved (improved)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep8 Train: 100%|██████████| 415/415 [03:56<00:00,  1.75it/s]\n",
            "Ep8 Val: 100%|██████████| 104/104 [00:39<00:00,  2.61it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep08  train_loss=1.0540  val_logloss=0.4007  acc=91.45%\n",
            "   ✅ best_model.pth saved (improved)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep9 Train: 100%|██████████| 415/415 [03:57<00:00,  1.75it/s]\n",
            "Ep9 Val: 100%|██████████| 104/104 [00:40<00:00,  2.57it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep09  train_loss=1.0405  val_logloss=0.3981  acc=91.57%\n",
            "   ✅ best_model.pth saved (improved)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep10 Train: 100%|██████████| 415/415 [03:57<00:00,  1.75it/s]\n",
            "Ep10 Val: 100%|██████████| 104/104 [00:40<00:00,  2.58it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep10  train_loss=1.0300  val_logloss=0.3938  acc=91.87%\n",
            "   ✅ best_model.pth saved (improved)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep11 Train: 100%|██████████| 415/415 [03:56<00:00,  1.75it/s]\n",
            "Ep11 Val: 100%|██████████| 104/104 [00:40<00:00,  2.56it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep11  train_loss=1.0255  val_logloss=0.3905  acc=92.21%\n",
            "   ✅ best_model.pth saved (improved)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep12 Train: 100%|██████████| 415/415 [03:56<00:00,  1.75it/s]\n",
            "Ep12 Val: 100%|██████████| 104/104 [00:40<00:00,  2.60it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep12  train_loss=1.0186  val_logloss=0.4046  acc=91.97%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep13 Train: 100%|██████████| 415/415 [03:55<00:00,  1.76it/s]\n",
            "Ep13 Val: 100%|██████████| 104/104 [00:40<00:00,  2.58it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep13  train_loss=1.0182  val_logloss=0.3774  acc=92.17%\n",
            "   ✅ best_model.pth saved (improved)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep14 Train: 100%|██████████| 415/415 [03:56<00:00,  1.75it/s]\n",
            "Ep14 Val: 100%|██████████| 104/104 [00:39<00:00,  2.60it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep14  train_loss=1.0122  val_logloss=0.3771  acc=92.53%\n",
            "   ✅ best_model.pth saved (improved)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep15 Train: 100%|██████████| 415/415 [03:56<00:00,  1.75it/s]\n",
            "Ep15 Val: 100%|██████████| 104/104 [00:39<00:00,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep15  train_loss=1.0042  val_logloss=0.3783  acc=92.41%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:3001: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}